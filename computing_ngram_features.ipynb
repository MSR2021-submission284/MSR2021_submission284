{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import scipy\n",
    "import javalang\n",
    "import re\n",
    "import sys\n",
    "import pyparsing\n",
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from pandas import DataFrame\n",
    "\n",
    "import os\n",
    "from os import listdir\n",
    "from os.path import isfile, join, splitext,split\n",
    "import csv\n",
    "import math\n",
    "import time\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import confusion_matrix, roc_curve, roc_auc_score\n",
    "\n",
    "from sklearn.feature_selection import RFE, VarianceThreshold, SelectKBest, f_classif, mutual_info_classif, chi2, f_regression, SelectFpr, SelectFdr\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.svm import SVR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Dense, Flatten, Conv1D, Conv2D, MaxPooling1D, MaxPooling2D, Dropout, Input, concatenate\n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk import bigrams, trigrams\n",
    "from collections import Counter, defaultdict\n",
    "from anytree import Node, RenderTree, PreOrderIter, PostOrderIter, LevelOrderIter\n",
    "from anytree.exporter import DotExporter\n",
    "import graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 0.5130"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_2/d/d_1_total.txt\",\"rb\") as fp:\n",
    "    train_1 = pickle.load(fp)\n",
    "with open(\"./data_2/d/d_2_total.txt\",\"rb\") as fp:\n",
    "    train_2 = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ngram_type:\n",
    "1: word ngrams\n",
    "\n",
    "2: structural ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_type = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AST_type = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining methods to count number of occurances of unigrams and bigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(threshold=sys.maxsize)\n",
    "\n",
    "def comment_remover(a):\n",
    "    a = re.sub(re.compile(\"//.*?\\n\"),\"\",a)\n",
    "    a = re.sub(re.compile(\"/\\*.*?\\*/\",re.DOTALL),\"\",a)\n",
    "    a = re.sub(re.compile(\"/\\*.*?\\n\",re.DOTALL),\"\",a)\n",
    "    a = re.sub(re.compile(\"\\*.*?\\n\",re.DOTALL),\"\",a)\n",
    "    a = re.sub(re.compile(\"/\\*.*?$\",re.DOTALL),\"\",a)\n",
    "    return(a)\n",
    "\n",
    "def preprocess(file_text):\n",
    "    file_text = comment_remover(file_text)\n",
    "    file_text_2 = file_text\n",
    "    file_text_2 = re.sub(r'[\"](.*?)[\"]','String_1',file_text_2)\n",
    "    \n",
    "    file_text_2 = re.sub(r'[\\'](.*?)[\\']','Literal_1',file_text_2)\n",
    "    \n",
    "    for a in ['\\\\','#','\"@@\"','\"','\\'']:\n",
    "        file_text_2 = file_text_2.replace(a,'')\n",
    "    file_text_2 = file_text_2.replace('`','')\n",
    "    return file_text_2\n",
    "\n",
    "def token_list(file_text):\n",
    "    tokens_list = javalang.tokenizer.tokenize(file_text)\n",
    "    tokens = []\n",
    "    for i in tokens_list:\n",
    "        class_name = i.__class__.__name__\n",
    "        value = i.value\n",
    "        if (value in ('Override','start_sentence','String_1','Literal_1')):\n",
    "            tokens.append(value)\n",
    "        elif (value in (';')):\n",
    "            tokens.append('end_sentence')\n",
    "        elif (value in ('.')):\n",
    "            tokens.append('dot')\n",
    "        elif (value in (',')):\n",
    "            tokens.append('comma')\n",
    "        elif (value in ('[')):\n",
    "            tokens.append('open_square_brackets')\n",
    "        elif (value in (']')):\n",
    "            tokens.append('closed_square_brackets')\n",
    "        elif (value in ('(','{')):\n",
    "            tokens.append('open_parenthesis')\n",
    "        elif (value in (')','}')):\n",
    "            tokens.append('closed_parenthesis')\n",
    "        elif (value in ('=','+=','-=','*=','/=','%=','<<=','>>=','&=','|=','^=')):\n",
    "            tokens.append('assignment_operator')\n",
    "        elif (value in ('+','-','*','/','%','++','--')):\n",
    "            tokens.append('arithmetic_operator')\n",
    "        elif (value in ('==','!=','>','<','>=','<=')):\n",
    "            tokens.append('relational_operator')\n",
    "        elif (value in ('&','|','^','~','<<','>>','>>>')):\n",
    "            tokens.append('bitwise_operator')\n",
    "        elif (value in ('&&','||','!')):\n",
    "            tokens.append('logical_operator')\n",
    "        elif (class_name in ('Identifier')):\n",
    "            if (len(value) == 1):\n",
    "                tokens.append('Identifier_1')\n",
    "            else:\n",
    "                tokens.append('Identifier_2')\n",
    "        elif ((class_name in ['Integer','DecimalInteger','FloatingPoint','DecimalFloatingPoint']) and (value[-1] not in ['L','l','e','E','f','F','d','D'])):\n",
    "                value_1 = float(value)\n",
    "                if (value_1 == 0):\n",
    "                    tokens.append('zero_number')\n",
    "                else:\n",
    "                    tokens.append('non_zero_number')\n",
    "        elif (class_name in class_types):\n",
    "            tokens.append(class_name)\n",
    "        elif (value in ('@')):\n",
    "            continue\n",
    "        else:\n",
    "            tokens.append(value)\n",
    "    return tokens\n",
    "\n",
    "def compute_counts(tokens):\n",
    "    for i in range(len(tokens)):\n",
    "        word_existed = 0\n",
    "        for j in range(len(words)):\n",
    "            if (tokens[i] == words[j]):\n",
    "                counts[j]+=1\n",
    "                word_existed = 1\n",
    "                break\n",
    "        if (word_existed == 0):\n",
    "            counts[n-1]+=1\n",
    "    return counts\n",
    "\n",
    "def compute_counts_2d(tokens):\n",
    "    for i in range(len(tokens)-1):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (tokens[i] == words[j]):\n",
    "                i_1 = j\n",
    "            if (tokens[i+1] == words[j]):\n",
    "                i_2 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        counts_2d[i_1,i_2] += 1\n",
    "    return counts_2d\n",
    "\n",
    "def compute_counts_3d(tokens):\n",
    "    for i in range(len(tokens)-2):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        i_3 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (tokens[i] == words[j]):\n",
    "                i_1 = j\n",
    "            if (tokens[i+1] == words[j]):\n",
    "                i_2 = j\n",
    "            if (tokens[i+2] == words[j]):\n",
    "                i_3 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        if (i_3 == -1):\n",
    "            i_3 = n-1\n",
    "        counts_3d[i_1,i_2,i_3] += 1\n",
    "    return counts_3d\n",
    "\n",
    "def compute_collective_counts(file):\n",
    "    l = len(file)\n",
    "    for i in range(l):\n",
    "        if True:\n",
    "            print('i:',i)\n",
    "            file_text = file[i][\"codes\"]\n",
    "            if (AST_type == 0 and ngram_type == 1):\n",
    "                file_text_2 = preprocess(file_text)\n",
    "                tokens = token_list(file_text_2)\n",
    "            else:\n",
    "                tree = javalang.parse.parse(file_text)\n",
    "                i_2 = 0\n",
    "                print_mode = 0\n",
    "                path_1 = []\n",
    "                tree_2 = create_tree(tree,i_2,path_1,print_mode)\n",
    "                if (AST_type == 1):\n",
    "                    tokens = [n.name for n in LevelOrderIter(tree_2)]\n",
    "                elif (AST_type == 2):\n",
    "                    tokens = [n.name for n in PreOrderIter(tree_2)]\n",
    "                elif (AST_type == 3):\n",
    "                    tokens = [n.name for n in PostOrderIter(tree_2)]\n",
    "            counts_array = compute_counts(tokens)\n",
    "\n",
    "            counts_array_2d = compute_counts_2d(tokens)\n",
    "            counts_array_3d = compute_counts_3d(tokens)\n",
    "    return(counts_array,counts_array_2d,counts_array_3d)\n",
    "\n",
    "def compute_unigram(counts):\n",
    "    # Add-one smoothing\n",
    "    smoothed_counts = counts + 1\n",
    "    unigram = []\n",
    "    unigram_array = []\n",
    "    for i in range(len(counts)):\n",
    "        probability = smoothed_counts[i]/(np.sum(counts)+len(words))\n",
    "        unigram.append([words[i],probability])\n",
    "        unigram_array.append(probability)\n",
    "    unigram = np.array(unigram)\n",
    "    unigram_array = np.array(unigram_array)\n",
    "    return unigram_array\n",
    "\n",
    "def compute_unigram_without_smoothing(counts):\n",
    "    unigram = []\n",
    "    unigram_array = []\n",
    "    for i in range(len(counts)):\n",
    "        probability = counts[i]/(np.sum(counts))\n",
    "        unigram.append([words[i],probability])\n",
    "        unigram_array.append(probability)\n",
    "    unigram = np.array(unigram)\n",
    "    unigram_array = np.array(unigram_array)\n",
    "    return unigram_array\n",
    "\n",
    "def compute_bigram(counts_2d,counts_1):\n",
    "    n = len(words)\n",
    "    bigram = np.zeros((n,n))\n",
    "    for i in range(len(counts_2d)):\n",
    "        for j in range(len(counts_2d)):\n",
    "            bigram[i,j] = (counts_2d[i,j]+1)/(counts_1[i]+len(words))\n",
    "    return bigram\n",
    "\n",
    "def compute_bigram_without_smoothing(counts_2d,counts_1):\n",
    "    n = len(words)\n",
    "    bigram = np.zeros((n,n))\n",
    "    for i in range(len(counts_2d)):\n",
    "        for j in range(len(counts_2d)):\n",
    "            if (counts_1[i] != 0):\n",
    "                bigram[i,j] = counts_2d[i,j]/counts_1[i]\n",
    "            else:\n",
    "                bigram[i,j] = 0\n",
    "    return bigram\n",
    "\n",
    "def compute_trigram_without_smoothing(counts_3d,counts_2d):\n",
    "    n = len(words)\n",
    "    trigram = np.zeros((n,n,n))\n",
    "    for i in range(len(counts_3d)):\n",
    "        for j in range(len(counts_3d)):\n",
    "            for k in range(len(counts_3d)):\n",
    "                if (counts_2d[i,j] != 0):\n",
    "                    trigram[i,j,k] = counts_3d[i,j,k]/counts_2d[i,j]\n",
    "                else:\n",
    "                    trigram[i,j,k] = 0\n",
    "    return trigram\n",
    "\n",
    "def compute_bigram_without_smoothing_d(counts_2d,counts_1):\n",
    "    d_bigram = np.zeros((n,n))\n",
    "    for element in dense_bigram_1:\n",
    "        i = int(element[0])\n",
    "        j = int(element[1])\n",
    "    \n",
    "        if (counts_1[i] != 0):\n",
    "            d_bigram[i,j] = counts_2d[i,j]/counts_1[i]\n",
    "        else:\n",
    "            d_bigram[i,j] = 0\n",
    "    return d_bigram\n",
    "\n",
    "def compute_trigram_without_smoothing_d(counts_3d,counts_2d):\n",
    "    d_trigram = np.zeros((n,n,n))\n",
    "    for element in dense_trigram_1:\n",
    "        i = int(element[0])\n",
    "        j = int(element[1])\n",
    "        k = int(element[2])\n",
    "    \n",
    "        if (counts_2d[i,j] != 0):\n",
    "            d_trigram[i,j,k] = counts_3d[i,j,k]/counts_2d[i,j]\n",
    "        else:\n",
    "            d_trigram[i,j,k] = 0\n",
    "    return d_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def children_list(node):\n",
    "    class_name = node.__class__\n",
    "    children_list_1 = super(class_name,node).children\n",
    "    return children_list_1\n",
    "    \n",
    "def print_line(file_text_1,position_line):\n",
    "    lines_to_print = range(position_line,position_line+1)\n",
    "    file_lines = file_text_1.splitlines()\n",
    "    print('Line of code:')\n",
    "    for a in lines_to_print:\n",
    "        print(file_lines[a-1])\n",
    "    return\n",
    "\n",
    "def append_words(paths_1):\n",
    "    for p in paths_1:\n",
    "        for token in p:\n",
    "            if token not in words:\n",
    "                words.append(token)\n",
    "\n",
    "def token_list_2(t):\n",
    "    p = []\n",
    "    p_class = []\n",
    "    for path, node in t:\n",
    "        if node not in p:\n",
    "            p.append(node)\n",
    "            p_class.append(node.__class__.__name__)\n",
    "    return p_class\n",
    "\n",
    "def bigram_list(tree,i,path_1,print_mode):\n",
    "    path = path_1.copy()\n",
    "    c = tree.__class__.__name__\n",
    "    if (c not in ['NoneType','str','list','set','bool'] and i != 0):\n",
    "        a_class = path[len(path)-1]\n",
    "        bigrams.append([a_class,c])\n",
    "    if (c not in ['NoneType','list']):\n",
    "        path.append(c)\n",
    "        i += 1\n",
    "    if (c not in ['NoneType','str','list','set','bool']):\n",
    "        dict_c = tree.__dict__\n",
    "        if (c != 'CompilationUnit' and print_mode == 1):\n",
    "            print('\\n'*5,'Class name:',c)\n",
    "            for j in dict_c:\n",
    "                print(j,':')\n",
    "                print(dict_c[j])\n",
    "                if (j == '_position'):\n",
    "                    position_line = dict_c[j].line\n",
    "                    position_column = dict_c[j].column\n",
    "                    print_line(file_text,position_line)\n",
    "                print('\\n')\n",
    "        nodes = children_list(tree)\n",
    "        for node in nodes:\n",
    "            bigram_list(node,i,path,print_mode)\n",
    "    else:\n",
    "        if (c in ['list','set']):\n",
    "            for element in tree:\n",
    "                bigram_list(element,i,path,print_mode)\n",
    "                \n",
    "def trigram_list(tree,i,path_1,print_mode):\n",
    "    path = path_1.copy()\n",
    "    c = tree.__class__.__name__\n",
    "    if (c not in ['NoneType','str','list','set','bool'] and i > 1):\n",
    "        a_class = path[len(path)-2]\n",
    "        b_class = path[len(path)-1]\n",
    "        trigrams.append([a_class,b_class,c])\n",
    "    if (c not in ['NoneType','list']):\n",
    "        path.append(c)\n",
    "        i += 1\n",
    "    if (c not in ['NoneType','str','list','set','bool']):\n",
    "        dict_c = tree.__dict__\n",
    "        if (c != 'CompilationUnit' and print_mode == 1):\n",
    "            print('\\n'*5,'Class name:',c)\n",
    "            for j in dict_c:\n",
    "                print(j,':')\n",
    "                print(dict_c[j])\n",
    "                if (j == '_position'):\n",
    "                    position_line = dict_c[j].line\n",
    "                    position_column = dict_c[j].column\n",
    "                    print_line(file_text,position_line)\n",
    "                print('\\n')\n",
    "        nodes = children_list(tree)\n",
    "        for node in nodes:\n",
    "            trigram_list(node,i,path,print_mode)\n",
    "    else:\n",
    "        if (c in ['list','set']):\n",
    "            for element in tree:\n",
    "                trigram_list(element,i,path,print_mode)\n",
    "                \n",
    "def compute_counts_2d_s(b):\n",
    "    for i in range(len(b)):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (b[i][0] == words[j]):\n",
    "                i_1 = j\n",
    "            if (b[i][1] == words[j]):\n",
    "                i_2 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        counts_2d[i_1,i_2] += 1\n",
    "    return counts_2d\n",
    "\n",
    "def compute_counts_3d_s(t):\n",
    "    for i in range(len(t)):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        i_3 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (t[i][0] == words[j]):\n",
    "                i_1 = j\n",
    "            if (t[i][1] == words[j]):\n",
    "                i_2 = j\n",
    "            if (t[i][2] == words[j]):\n",
    "                i_3 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        if (i_3 == -1):\n",
    "            i_3 = n-1\n",
    "        counts_3d[i_1,i_2,i_3] += 1\n",
    "    return counts_3d\n",
    "\n",
    "def compute_bigram_without_smoothing_s(counts_2d):\n",
    "    n = len(words)\n",
    "    bigram = np.zeros((n,n))\n",
    "    for i in range(len(counts_2d)):\n",
    "        for j in range(len(counts_2d)):\n",
    "            a = sum(counts_2d[i])\n",
    "            if (a != 0):\n",
    "                bigram[i,j] = counts_2d[i,j]/a\n",
    "            else:\n",
    "                bigram[i,j] = 0\n",
    "    return bigram\n",
    "\n",
    "def compute_trigram_without_smoothing_s(counts_3d):\n",
    "    n = len(words)\n",
    "    trigram = np.zeros((n,n,n))\n",
    "    for i in range(len(counts_3d)):\n",
    "        for j in range(len(counts_3d)):\n",
    "            for k in range(len(counts_3d)):\n",
    "                a = sum(counts_3d[i,j])\n",
    "                if (a != 0):\n",
    "                    trigram[i,j,k] = counts_3d[i,j,k]/a\n",
    "                else:\n",
    "                    trigram[i,j,k] = 0\n",
    "    return trigram\n",
    "\n",
    "def parse_m(s):\n",
    "    if not s.endswith(';'):\n",
    "        s = s + ';'\n",
    "    tokens = javalang.tokenizer.tokenize(s)\n",
    "    parser = javalang.parser.Parser(tokens)\n",
    "    m_tree = parser.parse_member_declaration()\n",
    "    return m_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining AST travesing methods:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tree(tree,i,path_1,print_mode):\n",
    "    path = path_1.copy()\n",
    "    c = tree.__class__.__name__\n",
    "    if (i == 0):\n",
    "        node_1 = Node(c)\n",
    "    if (c not in ['NoneType','str','list','set','bool'] and i != 0):\n",
    "        a_class = path[len(path)-1]\n",
    "        node_1 = Node(c, parent=a_class)\n",
    "    if (c not in ['NoneType','str','list','set','bool']):\n",
    "        path.append(node_1)\n",
    "        i += 1\n",
    "    if (c not in ['NoneType','str','list','set','bool']):\n",
    "        dict_c = tree.__dict__\n",
    "        if (c != 'CompilationUnit' and print_mode == 1):\n",
    "            print('\\n'*5,'Class name:',c)\n",
    "            for j in dict_c:\n",
    "                print(j,':')\n",
    "                print(dict_c[j])\n",
    "                if (j == '_position'):\n",
    "                    position_line = dict_c[j].line\n",
    "                    position_column = dict_c[j].column\n",
    "                    print_line(file_text,position_line)\n",
    "                print('\\n')\n",
    "        nodes = children_list(tree)\n",
    "        for node in nodes:\n",
    "            create_tree(node,i,path,print_mode)\n",
    "    else:\n",
    "        if (c in ['list','set']):\n",
    "            for element in tree:\n",
    "                create_tree(element,i,path,print_mode)\n",
    "    \n",
    "    if (c not in ['NoneType','str','list','set','bool'] and i == 1):\n",
    "        return node_1    \n",
    "\n",
    "def children_list(node):\n",
    "    class_name = node.__class__\n",
    "    children_list_1 = super(class_name,node).children\n",
    "    return children_list_1\n",
    " \n",
    "def print_tree(tree_1):\n",
    "#     print(RenderTree(tree_1))\n",
    "    tree_2 = RenderTree(tree_1)\n",
    "    for a_1,a_2,a_3 in tree_2:\n",
    "        print(\"%s%s\" % (a_1,a_3.name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Definig the list of words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_type = 1\n",
    "words = []\n",
    "if (ngram_type == 1):\n",
    "    class_types = ['Identifier_1','Identifier_2','zero_number','non_zero_number','Literal_1','Character','String_1','Boolean']\n",
    "    word_types = ['end_sentence','dot','comma','open_square_brackets','closed_square_brackets','open_parenthesis','closed_parenthesis','assignment_operator','arithmetic_operator','relational_operator','bitwise_operator','logical_operator','Override']\n",
    "    words_file = open('words.txt').read()\n",
    "    for i in word_types:\n",
    "        words.append(i)\n",
    "    for i in class_types:\n",
    "        words.append(i)\n",
    "    for i in words_file.split():\n",
    "        words.append(i)\n",
    "    words.append('other')\n",
    "\n",
    "elif (ngram_type == 2):\n",
    "    with open(\"./data_2/words.txt\",\"rb\") as fp:\n",
    "        words = pickle.load(fp)\n",
    "\n",
    "n = len(words)\n",
    "print('words:\\n',words)\n",
    "print('\\nnumber of words:',n)\n",
    "\n",
    "counts = np.zeros((n))\n",
    "counts_2d = np.zeros((n,n))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing collective counts (ngram_type = 1 or serial AST):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_type = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AST type => 0:words or parallel   1:BFS   2:pre-order   3:post-order\n",
    "AST_type = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = len(words)\n",
    "counts = np.zeros((n))\n",
    "counts_2d = np.zeros((n,n))\n",
    "counts_3d = np.zeros((n,n,n))\n",
    "calculated_values = compute_collective_counts(train_1)\n",
    "counts_array_1 = calculated_values[0]\n",
    "counts_array_2d_1 = calculated_values[1]\n",
    "counts_array_3d_1 = calculated_values[2]\n",
    "print('words list:\\n',words,'\\n')\n",
    "print(counts_array_1)\n",
    "print(counts_array_2d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts = np.zeros((n))\n",
    "counts_2d = np.zeros((n,n))\n",
    "counts_3d = np.zeros((n,n,n))\n",
    "calculated_values = compute_collective_counts(train_2)\n",
    "counts_array_2 = calculated_values[0]\n",
    "counts_array_2d_2 = calculated_values[1]\n",
    "counts_array_3d_2 = calculated_values[2]\n",
    "print('words list:\\n',words,'\\n')\n",
    "print(counts_array_2)\n",
    "print(counts_array_2d_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unigram_1 = compute_unigram_without_smoothing(counts_array_1)\n",
    "\n",
    "unigram_2 = compute_unigram_without_smoothing(counts_array_2)\n",
    "\n",
    "bigram_1 = compute_bigram_without_smoothing(counts_array_2d_1,counts_array_1)\n",
    "\n",
    "bigram_2 = compute_bigram_without_smoothing(counts_array_2d_2,counts_array_2)\n",
    "\n",
    "print('words list:\\n',words,'\\n')\n",
    "trigram_1 = compute_trigram_without_smoothing(counts_array_3d_1,counts_array_2d_1)\n",
    "print('trigram_1:\\n',trigram_1[4])\n",
    "\n",
    "print('words list:\\n',words,'\\n')\n",
    "trigram_2 = compute_trigram_without_smoothing(counts_array_3d_2,counts_array_2d_2)\n",
    "print('trigram_2:\\n',trigram_2[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "np.save('./data_2/unigram_1.npy',unigram_1)\n",
    "np.save('./data_2/unigram_2.npy',unigram_2)\n",
    "\n",
    "np.save('./data_2/bigram_1.npy',bigram_1)\n",
    "np.save('./data_2/bigram_2.npy',bigram_2)\n",
    "\n",
    "np.save('./data_2/trigram_1.npy',trigram_1)\n",
    "np.save('./data_2/trigram_2.npy',trigram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "np.save('./data_2/unigram_1_m.npy',unigram_1)\n",
    "np.save('./data_2/unigram_2_m.npy',unigram_2)\n",
    "\n",
    "np.save('./data_2/bigram_1_m.npy',bigram_1)\n",
    "np.save('./data_2/bigram_2_m.npy',bigram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "unigram_1 = np.load('./data_2/unigram_1.npy')\n",
    "unigram_2 = np.load('./data_2/unigram_2.npy')\n",
    "\n",
    "bigram_1 = np.load('./data_2/bigram_1.npy')\n",
    "bigram_2 = np.load('./data_2/bigram_2.npy')\n",
    "\n",
    "trigram_1 = np.load('./data_2/trigram_1.npy')\n",
    "trigram_2 = np.load('./data_2/trigram_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "unigram_1 = np.load('./data_2/unigram_1_m.npy')\n",
    "unigram_2 = np.load('./data_2/unigram_2_m.npy')\n",
    "\n",
    "bigram_1 = np.load('./data_2/bigram_1_m.npy')\n",
    "bigram_2 = np.load('./data_2/bigram_2_m.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing collective counts (ngram_type = 2):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_type = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# files\n",
    "n = len(words)\n",
    "length = len(train_1)\n",
    "counts = np.zeros((n))\n",
    "counts_2d = np.zeros((n,n))\n",
    "counts_3d = np.zeros((n,n,n))\n",
    "\n",
    "for i in range(length):\n",
    "    if True:\n",
    "        print(i)\n",
    "        file_text = train_1[i][\"codes\"]\n",
    "        tree = javalang.parse.parse(file_text)\n",
    "        tokens = token_list_2(tree)\n",
    "        d = 0\n",
    "        print_mode = 0\n",
    "        path_1 = []\n",
    "        bigrams = []\n",
    "        bigram_list(tree,d,path_1,print_mode)\n",
    "        s_counts_array_1 = compute_counts(tokens)\n",
    "        s_counts_array_2d_1 = compute_counts_2d_s(bigrams)\n",
    "        \n",
    "        d = 0\n",
    "        path_1 = []\n",
    "        trigrams = []\n",
    "        trigram_list(tree,d,path_1,print_mode)\n",
    "        s_counts_array_3d_1 = compute_counts_3d_s(trigrams)\n",
    "\n",
    "print('words list:\\n',words,'\\n')\n",
    "print(s_counts_array_1)\n",
    "print(s_counts_array_2d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "n = len(words)\n",
    "length = len(train_1)\n",
    "counts = np.zeros((n))\n",
    "counts_2d = np.zeros((n,n))\n",
    "\n",
    "for i in range(length):\n",
    "    if True:\n",
    "        print(i)\n",
    "        file_text = train_1[i][\"codes\"]\n",
    "        tree = parse_m(file_text)\n",
    "        tokens = token_list_2(tree)\n",
    "        d = 0\n",
    "        print_mode = 0\n",
    "        path_1 = []\n",
    "        bigrams = []\n",
    "        bigram_list(tree,d,path_1,print_mode)\n",
    "        s_counts_array_1 = compute_counts(tokens)\n",
    "        s_counts_array_2d_1 = compute_counts_2d_s(bigrams)\n",
    "\n",
    "print('words list:\\n',words,'\\n')\n",
    "print(s_counts_array_1)\n",
    "print(s_counts_array_2d_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "n = len(words)\n",
    "length = len(train_2)\n",
    "counts = np.zeros((n))\n",
    "counts_2d = np.zeros((n,n))\n",
    "counts_3d = np.zeros((n,n,n))\n",
    "\n",
    "for i in range(length):\n",
    "    if True:\n",
    "        print(i)\n",
    "        file_text = train_2[i][\"codes\"]\n",
    "        tree = javalang.parse.parse(file_text)\n",
    "        tokens = token_list_2(tree)\n",
    "        d = 0\n",
    "        print_mode = 0\n",
    "        path_1 = []\n",
    "        bigrams = []\n",
    "        bigram_list(tree,d,path_1,print_mode)\n",
    "        s_counts_array_2 = compute_counts(tokens)\n",
    "        s_counts_array_2d_2 = compute_counts_2d_s(bigrams)\n",
    "        \n",
    "        d = 0\n",
    "        path_1 = []\n",
    "        trigrams = []\n",
    "        trigram_list(tree,d,path_1,print_mode)\n",
    "        s_counts_array_3d_2 = compute_counts_3d_s(trigrams)\n",
    "\n",
    "\n",
    "print('words list:\\n',words,'\\n')\n",
    "print(s_counts_array_2)\n",
    "print(s_counts_array_2d_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "n = len(words)\n",
    "length = len(train_2)\n",
    "counts = np.zeros((n))\n",
    "counts_2d = np.zeros((n,n))\n",
    "\n",
    "for i in range(length):\n",
    "    if True:\n",
    "        print(i)\n",
    "        file_text = train_2[i][\"codes\"]\n",
    "        tree = parse_m(file_text)\n",
    "        tokens = token_list_2(tree)\n",
    "        d = 0\n",
    "        print_mode = 0\n",
    "        path_1 = []\n",
    "        bigrams = []\n",
    "        bigram_list(tree,d,path_1,print_mode)\n",
    "        s_counts_array_2 = compute_counts(tokens)\n",
    "        s_counts_array_2d_2 = compute_counts_2d_s(bigrams)\n",
    "\n",
    "print('words list:\\n',words,'\\n')\n",
    "print(s_counts_array_2)\n",
    "print(s_counts_array_2d_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_unigram_1 = compute_unigram_without_smoothing(s_counts_array_1)\n",
    "\n",
    "s_unigram_2 = compute_unigram_without_smoothing(s_counts_array_2)\n",
    "\n",
    "s_bigram_1 = compute_bigram_without_smoothing_s(s_counts_array_2d_1)\n",
    "\n",
    "s_bigram_2 = compute_bigram_without_smoothing_s(s_counts_array_2d_2)\n",
    "\n",
    "print('words list:\\n',words,'\\n')\n",
    "s_trigram_1 = compute_trigram_without_smoothing_s(s_counts_array_3d_1)\n",
    "print('trigram_1:\\n',s_trigram_1[4])\n",
    "\n",
    "print('words list:\\n',words,'\\n')\n",
    "s_trigram_2 = compute_trigram_without_smoothing_s(s_counts_array_3d_2)\n",
    "print('trigram_2:\\n',s_trigram_2[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "np.save('./data_2/s_unigram_1.npy',s_unigram_1)\n",
    "np.save('./data_2/s_unigram_2.npy',s_unigram_2)\n",
    "\n",
    "np.save('./data_2/s_bigram_1.npy',s_bigram_1)\n",
    "np.save('./data_2/s_bigram_2.npy',s_bigram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "np.save('./data_2/s_unigram_1_m.npy',s_unigram_1)\n",
    "np.save('./data_2/s_unigram_2_m.npy',s_unigram_2)\n",
    "\n",
    "np.save('./data_2/s_bigram_1_m.npy',s_bigram_1)\n",
    "np.save('./data_2/s_bigram_2_m.npy',s_bigram_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "s_unigram_1 = np.load('./data_2/s_unigram_1.npy')\n",
    "s_unigram_2 = np.load('./data_2/s_unigram_2.npy')\n",
    "\n",
    "s_bigram_1 = np.load('./data_2/s_bigram_1.npy')\n",
    "s_bigram_2 = np.load('./data_2/s_bigram_2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "s_unigram_1 = np.load('./data_2/s_unigram_1_m.npy')\n",
    "s_unigram_2 = np.load('./data_2/s_unigram_2_m.npy')\n",
    "\n",
    "s_bigram_1 = np.load('./data_2/s_bigram_1_m.npy')\n",
    "s_bigram_2 = np.load('./data_2/s_bigram_2_m.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Storing indices of non_zero bigrams (for calculating a dense bigram array):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "AST_type = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_type = 2\n",
    "if (ngram_type == 1 or AST_type != 0):\n",
    "    b_1 = bigram_1.copy()\n",
    "    b_2 = bigram_2.copy()\n",
    "elif (ngram_type == 2):\n",
    "    b_1 = s_bigram_1.copy()\n",
    "    b_2 = s_bigram_2.copy()\n",
    "\n",
    "dense_bigram_1 = []\n",
    "dense_bigram_2 = []\n",
    "\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        if (b_1[i,j]!=0 and b_2[i,j]!=0):\n",
    "            dense_bigram_1.append([i,j,b_1[i,j]])\n",
    "            dense_bigram_2.append([i,j,b_2[i,j]])\n",
    "\n",
    "dense_bigram_1 = np.array(dense_bigram_1)\n",
    "dense_bigram_2 = np.array(dense_bigram_2)\n",
    "\n",
    "print(len(dense_bigram_1))\n",
    "print(len(dense_bigram_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data_2/dense_bigram_1_AST4.npy',dense_bigram_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "if (ngram_type == 1):\n",
    "    np.save('./data_2/dense_bigram_1.npy',dense_bigram_1)\n",
    "if (ngram_type == 2):\n",
    "    np.save('./data_2/s_dense_bigram_1.npy',dense_bigram_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "if (ngram_type == 1):\n",
    "    np.save('./data_2/dense_bigram_1_m.npy',dense_bigram_1)\n",
    "if (ngram_type == 2):\n",
    "    np.save('./data_2/s_dense_bigram_1_m.npy',dense_bigram_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_bigram_1 = np.load('./data_2/dense_bigram_1_AST4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "dense_bigram_1 = np.load('./data_2/dense_bigram_1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "dense_bigram_1 = np.load('./data_2/dense_bigram_1_m.npy')\n",
    "s_dense_bigram_1 = np.load('./data_2/s_dense_bigram_1_m.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing dense trigrams:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams\n",
    "ngram_type = 1\n",
    "if (ngram_type == 1 or AST_type != 0):\n",
    "    t_1 = trigram_1.copy()\n",
    "    t_2 = trigram_2.copy()\n",
    "elif (ngram_type == 2):\n",
    "    t_1 = s_trigram_1.copy()\n",
    "    t_2 = s_trigram_2.copy()\n",
    "\n",
    "dense_trigram_1 = []\n",
    "dense_trigram_2 = []\n",
    "\n",
    "for i in range(len(words)):\n",
    "    for j in range(len(words)):\n",
    "        for k in range(len(words)):\n",
    "            if (t_1[i,j,k]!=0 and t_2[i,j,k]!=0):\n",
    "                dense_trigram_1.append([i,j,k,t_1[i,j,k]])\n",
    "                dense_trigram_2.append([i,j,k,t_2[i,j,k]])\n",
    "\n",
    "dense_trigram_1 = np.array(dense_trigram_1)\n",
    "dense_trigram_2 = np.array(dense_trigram_2)\n",
    "\n",
    "print(len(dense_trigram_1))\n",
    "print(len(dense_trigram_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data_2/dense_trigram_1_AST4.npy',dense_trigram_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "if (ngram_type == 1):\n",
    "    np.save('./data_2/dense_trigram_1.npy',dense_trigram_1)\n",
    "if (ngram_type == 2):\n",
    "    np.save('./data_2/s_dense_trigram_1.npy',dense_trigram_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense_trigram_1 = np.load('./data_2/dense_trigram_1_AST4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "dense_trigram_1 = np.load('./data_2/dense_trigram_1.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing ngrams probability & count for each text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = train_1 + train_2\n",
    "print(len(data))\n",
    "print(data[100][\"is_buggy\"])\n",
    "random.Random(0).shuffle(data)\n",
    "print(data[4][\"is_buggy\"])\n",
    "print(data[0][\"codes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_2/d_3.txt\",\"wb\") as fp:\n",
    "    pickle.dump(data, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data_2/d_3.txt\",\"rb\") as fp:\n",
    "    data = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts_file(tokens):\n",
    "    counts = np.zeros((n))\n",
    "    for i in range(len(tokens)):\n",
    "        word_existed = 0\n",
    "        for j in range(len(words)):\n",
    "            if (tokens[i] == words[j]):\n",
    "                counts[j]+=1\n",
    "                word_existed = 1\n",
    "                break\n",
    "        if (word_existed == 0):\n",
    "            counts[n-1]+=1\n",
    "    return counts\n",
    "\n",
    "def compute_counts_2d_file(tokens):\n",
    "    counts_2d = np.zeros((n,n))\n",
    "    for i in range(len(tokens)-1):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (tokens[i] == words[j]):\n",
    "                i_1 = j\n",
    "            if (tokens[i+1] == words[j]):\n",
    "                i_2 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        counts_2d[i_1,i_2] += 1\n",
    "    return counts_2d\n",
    "\n",
    "def compute_counts_2d_file_s(b):\n",
    "    counts_2d = np.zeros((n,n))\n",
    "    for i in range(len(b)):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (b[i][0] == words[j]):\n",
    "                i_1 = j\n",
    "            if (b[i][1] == words[j]):\n",
    "                i_2 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        counts_2d[i_1,i_2] += 1\n",
    "    return counts_2d\n",
    "\n",
    "def compute_counts_3d_file(tokens):\n",
    "    counts_3d = np.zeros((n,n,n))\n",
    "    for i in range(len(tokens)-2):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        i_3 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (tokens[i] == words[j]):\n",
    "                i_1 = j\n",
    "            if (tokens[i+1] == words[j]):\n",
    "                i_2 = j\n",
    "            if (tokens[i+2] == words[j]):\n",
    "                i_3 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        if (i_3 == -1):\n",
    "            i_3 = n-1\n",
    "        counts_3d[i_1,i_2,i_3] += 1\n",
    "    return counts_3d\n",
    "\n",
    "def compute_counts_3d_file_s(t):\n",
    "    counts_3d = np.zeros((n,n,n))\n",
    "    for i in range(len(t)):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        i_3 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (t[i][0] == words[j]):\n",
    "                i_1 = j\n",
    "            if (t[i][1] == words[j]):\n",
    "                i_2 = j\n",
    "            if (t[i][2] == words[j]):\n",
    "                i_3 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        if (i_3 == -1):\n",
    "            i_3 = n-1\n",
    "        counts_3d[i_1,i_2,i_3] += 1\n",
    "    return counts_3d"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing ngrams probability & count for each text file (ngram_type = 1 or serial AST):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AST_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ngram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "n = len(words)\n",
    "unigram_data = []\n",
    "bigram_data = []\n",
    "dense_bigram_data = []\n",
    "dense_trigram_data = []\n",
    "\n",
    "unigram_data_c = []\n",
    "bigram_data_c = []\n",
    "dense_bigram_data_c = []\n",
    "dense_trigram_data_c = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if True:\n",
    "        print(i)\n",
    "        counts = np.zeros((n))\n",
    "        counts_2d = np.zeros((n,n))\n",
    "        counts_3d = np.zeros((n,n,n))\n",
    "        file_text = data[i][\"codes\"]\n",
    "    \n",
    "        if (AST_type == 0 and ngram_type == 1):\n",
    "            p_file_text = preprocess(file_text)\n",
    "            tokens = token_list(p_file_text)\n",
    "        else:\n",
    "            tree = javalang.parse.parse(file_text)\n",
    "            i_2 = 0\n",
    "            print_mode = 0\n",
    "            path_1 = []\n",
    "            tree_2 = create_tree(tree,i_2,path_1,print_mode)\n",
    "            if (AST_type == 1):\n",
    "                tokens = [n.name for n in LevelOrderIter(tree_2)]\n",
    "            elif (AST_type == 2):\n",
    "                tokens = [n.name for n in PreOrderIter(tree_2)]\n",
    "            elif (AST_type == 3):\n",
    "                tokens = [n.name for n in PostOrderIter(tree_2)]\n",
    "    \n",
    "        counts = compute_counts_file(tokens)\n",
    "        counts_2d = compute_counts_2d_file(tokens)\n",
    "        counts_3d = compute_counts_3d_file(tokens)\n",
    "        unigram = compute_unigram_without_smoothing(counts)\n",
    "        bigram = compute_bigram_without_smoothing(counts_2d,counts)\n",
    "        trigram = compute_trigram_without_smoothing(counts_3d,counts_2d)\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        c = []\n",
    "        d = []\n",
    "        \n",
    "        a_c = []\n",
    "        b_c = []\n",
    "        c_c = []\n",
    "        d_c = []\n",
    "        \n",
    "        for k in range(len(dense_bigram_1)):\n",
    "            k_1 = int(dense_bigram_1[k,0])\n",
    "            k_2 = int(dense_bigram_1[k,1])\n",
    "            \n",
    "            c.append(bigram[k_1,k_2])\n",
    "            c_c.append(counts_2d[k_1,k_2])\n",
    "            \n",
    "        for k in range(len(dense_trigram_1)):\n",
    "            k_1 = int(dense_trigram_1[k,0])\n",
    "            k_2 = int(dense_trigram_1[k,1])\n",
    "            k_3 = int(dense_trigram_1[k,2])\n",
    "            \n",
    "            d.append(trigram[k_1,k_2,k_3])\n",
    "            d_c.append(counts_3d[k_1,k_2,k_3])\n",
    "           \n",
    "        a = unigram\n",
    "        b = bigram.flatten()\n",
    "        c = np.array(c)\n",
    "        d = np.array(d)\n",
    "        \n",
    "        a_c = counts\n",
    "        b_c = counts_2d.flatten()\n",
    "        c_c = np.array(c_c)\n",
    "        d_c = np.array(d_c)\n",
    "        \n",
    "        if (data[i][\"is_buggy\"]==True):\n",
    "            a = np.append(a,1)\n",
    "            b = np.append(b,1)\n",
    "            c = np.append(c,1)\n",
    "            d = np.append(d,1)\n",
    "            \n",
    "            a_c = np.append(a_c,1)\n",
    "            b_c = np.append(b_c,1)\n",
    "            c_c = np.append(c_c,1)\n",
    "            d_c = np.append(d_c,1)\n",
    "        \n",
    "        else:\n",
    "            a = np.append(a,0)\n",
    "            b = np.append(b,0)\n",
    "            c = np.append(c,0)\n",
    "            d = np.append(d,0)\n",
    "            \n",
    "            a_c = np.append(a_c,0)\n",
    "            b_c = np.append(b_c,0)\n",
    "            c_c = np.append(c_c,0)\n",
    "            d_c = np.append(d_c,0)\n",
    "\n",
    "        unigram_data.append(a)\n",
    "        bigram_data.append(b)\n",
    "        dense_bigram_data.append(c)\n",
    "        dense_trigram_data.append(d)\n",
    "        \n",
    "        unigram_data_c.append(a_c)\n",
    "        bigram_data_c.append(b_c)\n",
    "        dense_bigram_data_c.append(c_c)\n",
    "        dense_trigram_data_c.append(d_c)\n",
    "                \n",
    "unigram_data = np.array(unigram_data)\n",
    "dense_bigram_data = np.array(dense_bigram_data)\n",
    "dense_trigram_data = np.array(dense_trigram_data)\n",
    "\n",
    "unigram_data_c = np.array(unigram_data_c)\n",
    "dense_bigram_data_c = np.array(dense_bigram_data_c)\n",
    "dense_trigram_data_c = np.array(dense_trigram_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Time\n",
    "# files\n",
    "n = len(words)\n",
    "unigram_data = []\n",
    "bigram_data = []\n",
    "dense_bigram_data = []\n",
    "dense_trigram_data = []\n",
    "\n",
    "unigram_data_c = []\n",
    "bigram_data_c = []\n",
    "dense_bigram_data_c = []\n",
    "dense_trigram_data_c = []\n",
    "\n",
    "bigram_computing_time = 0\n",
    "trigram_computing_time = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if True:\n",
    "        print(i)\n",
    "        counts = np.zeros((n))\n",
    "        counts_2d = np.zeros((n,n))\n",
    "        counts_3d = np.zeros((n,n,n))\n",
    "        file_text = data[i][\"codes\"]\n",
    "    \n",
    "        if (AST_type == 0 and ngram_type == 1):\n",
    "            p_file_text = preprocess(file_text)\n",
    "            tokens = token_list(p_file_text)\n",
    "        else:\n",
    "            tree = javalang.parse.parse(file_text)\n",
    "            i_2 = 0\n",
    "            print_mode = 0\n",
    "            path_1 = []\n",
    "            tree_2 = create_tree(tree,i_2,path_1,print_mode)\n",
    "            if (AST_type == 1):\n",
    "                tokens = [n.name for n in LevelOrderIter(tree_2)]\n",
    "            elif (AST_type == 2):\n",
    "                tokens = [n.name for n in PreOrderIter(tree_2)]\n",
    "            elif (AST_type == 3):\n",
    "                tokens = [n.name for n in PostOrderIter(tree_2)]\n",
    "    \n",
    "        counts = compute_counts_file(tokens)\n",
    "        counts_2d = compute_counts_2d_file(tokens)\n",
    "        counts_3d = compute_counts_3d_file(tokens)\n",
    "        unigram = compute_unigram_without_smoothing(counts)\n",
    "        \n",
    "        b_1_t = time.time()\n",
    "        bigram = compute_bigram_without_smoothing_d(counts_2d,counts)\n",
    "        b_2_t = time.time()\n",
    "        bigram_computing_time += (b_2_t -b_1_t)\n",
    "        \n",
    "        t_1_t = time.time()\n",
    "        trigram = compute_trigram_without_smoothing_d(counts_3d,counts_2d)\n",
    "        t_2_t = time.time()\n",
    "        trigram_computing_time += (t_2_t -t_1_t)\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        c = []\n",
    "        d = []\n",
    "        \n",
    "        a_c = []\n",
    "        b_c = []\n",
    "        c_c = []\n",
    "        d_c = []\n",
    "        \n",
    "        for k in range(len(dense_bigram_1)):\n",
    "            k_1 = int(dense_bigram_1[k,0])\n",
    "            k_2 = int(dense_bigram_1[k,1])\n",
    "            \n",
    "            c.append(bigram[k_1,k_2])\n",
    "            c_c.append(counts_2d[k_1,k_2])\n",
    "            \n",
    "        for k in range(len(dense_trigram_1)):\n",
    "            k_1 = int(dense_trigram_1[k,0])\n",
    "            k_2 = int(dense_trigram_1[k,1])\n",
    "            k_3 = int(dense_trigram_1[k,2])\n",
    "            \n",
    "            d.append(trigram[k_1,k_2,k_3])\n",
    "            d_c.append(counts_3d[k_1,k_2,k_3])\n",
    "           \n",
    "        a = unigram\n",
    "        b = bigram.flatten()\n",
    "        c = np.array(c)\n",
    "        d = np.array(d)\n",
    "        \n",
    "        a_c = counts\n",
    "        b_c = counts_2d.flatten()\n",
    "        c_c = np.array(c_c)\n",
    "        d_c = np.array(d_c)\n",
    "        \n",
    "        if (data[i][\"is_buggy\"]==True):\n",
    "            a = np.append(a,1)\n",
    "            b = np.append(b,1)\n",
    "            c = np.append(c,1)\n",
    "            d = np.append(d,1)\n",
    "            \n",
    "            a_c = np.append(a_c,1)\n",
    "            b_c = np.append(b_c,1)\n",
    "            c_c = np.append(c_c,1)\n",
    "            d_c = np.append(d_c,1)\n",
    "        \n",
    "        else:\n",
    "            a = np.append(a,0)\n",
    "            b = np.append(b,0)\n",
    "            c = np.append(c,0)\n",
    "            d = np.append(d,0)\n",
    "            \n",
    "            a_c = np.append(a_c,0)\n",
    "            b_c = np.append(b_c,0)\n",
    "            c_c = np.append(c_c,0)\n",
    "            d_c = np.append(d_c,0)\n",
    "\n",
    "        unigram_data.append(a)\n",
    "        bigram_data.append(b)\n",
    "        dense_bigram_data.append(c)\n",
    "        dense_trigram_data.append(d)\n",
    "        \n",
    "        unigram_data_c.append(a_c)\n",
    "        bigram_data_c.append(b_c)\n",
    "        dense_bigram_data_c.append(c_c)\n",
    "        dense_trigram_data_c.append(d_c)\n",
    "                \n",
    "unigram_data = np.array(unigram_data)\n",
    "dense_bigram_data = np.array(dense_bigram_data)\n",
    "dense_trigram_data = np.array(dense_trigram_data)\n",
    "\n",
    "unigram_data_c = np.array(unigram_data_c)\n",
    "dense_bigram_data_c = np.array(dense_bigram_data_c)\n",
    "dense_trigram_data_c = np.array(dense_trigram_data_c)\n",
    "\n",
    "print('bigram computing time:',round(bigram_computing_time,4))\n",
    "print('trigram computing time:',round(trigram_computing_time,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "n = len(words)\n",
    "unigram_data = []\n",
    "bigram_data = []\n",
    "dense_bigram_data = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if True:\n",
    "        print(i)\n",
    "        counts = np.zeros((n))\n",
    "        counts_2d = np.zeros((n,n))\n",
    "        file_text = data[i][\"codes\"]\n",
    "        p_file_text = preprocess(file_text)\n",
    "        tokens = token_list(p_file_text)\n",
    "        counts = compute_counts_file(tokens)\n",
    "        counts_2d = compute_counts_2d_file(tokens)\n",
    "        unigram = compute_unigram_without_smoothing(counts)\n",
    "        bigram = compute_bigram_without_smoothing(counts_2d,counts)\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        c = []\n",
    "        for k in range(len(dense_bigram_1)):\n",
    "            k_1 = int(dense_bigram_1[k,0])\n",
    "            k_2 = int(dense_bigram_1[k,1])\n",
    "            c.append(bigram[k_1,k_2])\n",
    "           \n",
    "        a = unigram\n",
    "        b = bigram.flatten()\n",
    "        c = np.array(c)\n",
    "        \n",
    "        if (data[i][\"is_buggy\"]==True):\n",
    "            a = np.append(a,1)\n",
    "            b = np.append(b,1)\n",
    "            c = np.append(c,1)\n",
    "        else:\n",
    "            a = np.append(a,0)\n",
    "            b = np.append(b,0)\n",
    "            c = np.append(c,0)\n",
    "\n",
    "        unigram_data.append(a)\n",
    "        bigram_data.append(b)\n",
    "        dense_bigram_data.append(c)\n",
    "                \n",
    "unigram_data = np.array(unigram_data)\n",
    "dense_bigram_data = np.array(dense_bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (words)\n",
    "np.save('./data_3/unigram_data.npy',unigram_data)\n",
    "np.save('./data_3/dense_bigram_data.npy',dense_bigram_data)\n",
    "np.save('./data_3/dense_trigram_data.npy',dense_trigram_data)\n",
    "\n",
    "np.save('./data_3/unigram_data_c.npy',unigram_data_c)\n",
    "np.save('./data_3/dense_bigram_data_c.npy',dense_bigram_data_c)\n",
    "np.save('./data_3/dense_trigram_data_c.npy',dense_trigram_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST1)\n",
    "np.save('./data_2/unigram_data_AST1.npy',unigram_data)\n",
    "np.save('./data_2/dense_bigram_data_AST1.npy',dense_bigram_data)\n",
    "np.save('./data_2/dense_trigram_data_AST1.npy',dense_trigram_data)\n",
    "\n",
    "np.save('./data_2/unigram_data_c_AST1.npy',unigram_data_c)\n",
    "np.save('./data_2/dense_bigram_data_c_AST1.npy',dense_bigram_data_c)\n",
    "np.save('./data_2/dense_trigram_data_c_AST1.npy',dense_trigram_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST2)\n",
    "np.save('./data_2/unigram_data_AST2.npy',unigram_data)\n",
    "np.save('./data_2/dense_bigram_data_AST2.npy',dense_bigram_data)\n",
    "np.save('./data_2/dense_trigram_data_AST2.npy',dense_trigram_data)\n",
    "\n",
    "np.save('./data_2/unigram_data_c_AST2.npy',unigram_data_c)\n",
    "np.save('./data_2/dense_bigram_data_c_AST2.npy',dense_bigram_data_c)\n",
    "np.save('./data_2/dense_trigram_data_c_AST2.npy',dense_trigram_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST3)\n",
    "np.save('./data_2/unigram_data_AST3.npy',unigram_data)\n",
    "np.save('./data_2/dense_bigram_data_AST3.npy',dense_bigram_data)\n",
    "np.save('./data_2/dense_trigram_data_AST3.npy',dense_trigram_data)\n",
    "\n",
    "np.save('./data_2/unigram_data_c_AST3.npy',unigram_data_c)\n",
    "np.save('./data_2/dense_bigram_data_c_AST3.npy',dense_bigram_data_c)\n",
    "np.save('./data_2/dense_trigram_data_c_AST3.npy',dense_trigram_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "np.save('./data_2/unigram_data_m.npy',unigram_data)\n",
    "np.save('./data_2/dense_bigram_data_m.npy',dense_bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (words)\n",
    "unigram_data = np.load('./data_2/unigram_data.npy')\n",
    "dense_bigram_data = np.load('./data_2/dense_bigram_data.npy')\n",
    "dense_trigram_data = np.load('./data_2/dense_trigram_data.npy')\n",
    "\n",
    "unigram_data_c = np.load('./data_2/unigram_data_c.npy')\n",
    "dense_bigram_data_c = np.load('./data_2/dense_bigram_data_c.npy')\n",
    "dense_trigram_data_c = np.load('./data_2/dense_trigram_data_c.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST1)\n",
    "unigram_data = np.load('./data_2/unigram_data_AST1.npy')\n",
    "dense_bigram_data = np.load('./data_2/dense_bigram_data_AST1.npy')\n",
    "dense_trigram_data = np.load('./data_2/dense_trigram_data_AST1.npy')\n",
    "\n",
    "unigram_data_c = np.load('./data_2/unigram_data_c_AST1.npy')\n",
    "dense_bigram_data_c = np.load('./data_2/dense_bigram_data_c_AST1.npy')\n",
    "dense_trigram_data_c = np.load('./data_2/dense_trigram_data_c_AST1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST2)\n",
    "unigram_data = np.load('./data_2/unigram_data_AST2.npy')\n",
    "dense_bigram_data = np.load('./data_2/dense_bigram_data_AST2.npy')\n",
    "dense_trigram_data = np.load('./data_2/dense_trigram_data_AST2.npy')\n",
    "\n",
    "unigram_data_c = np.load('./data_2/unigram_data_c_AST2.npy')\n",
    "dense_bigram_data_c = np.load('./data_2/dense_bigram_data_c_AST2.npy')\n",
    "dense_trigram_data_c = np.load('./data_2/dense_trigram_data_c_AST2.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST3)\n",
    "unigram_data = np.load('./data_2/unigram_data_AST3.npy')\n",
    "dense_bigram_data = np.load('./data_2/dense_bigram_data_AST3.npy')\n",
    "dense_trigram_data = np.load('./data_2/dense_trigram_data_AST3.npy')\n",
    "\n",
    "unigram_data_c = np.load('./data_2/unigram_data_c_AST3.npy')\n",
    "dense_bigram_data_c = np.load('./data_2/dense_bigram_data_c_AST3.npy')\n",
    "dense_trigram_data_c = np.load('./data_2/dense_trigram_data_c_AST3.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "unigram_data = np.load('./data_2/unigram_data_m.npy')\n",
    "dense_bigram_data = np.load('./data_2/dense_bigram_data_m.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing ngrams probability & count for each text file (path-based AST):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AST_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ngram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# files\n",
    "n = len(words)\n",
    "unigram_data = []\n",
    "bigram_data = []\n",
    "dense_bigram_data = []\n",
    "dense_trigram_data = []\n",
    "\n",
    "unigram_data_c = []\n",
    "bigram_data_c = []\n",
    "dense_bigram_data_c = []\n",
    "dense_trigram_data_c = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if True:\n",
    "        print(i)\n",
    "        counts = np.zeros((n))\n",
    "        counts_2d = np.zeros((n,n))\n",
    "        counts_3d = np.zeros((n,n,n))\n",
    "        file_text = data[i][\"codes\"]\n",
    "        tree = javalang.parse.parse(file_text)\n",
    "        tokens = token_list_2(tree)\n",
    "        d = 0\n",
    "        print_mode = 0\n",
    "        path_1 = []\n",
    "        bigrams = []\n",
    "        bigram_list(tree,d,path_1,print_mode)\n",
    "        \n",
    "        d = 0\n",
    "        path_1 = []\n",
    "        trigrams = []\n",
    "        print_mode = 0\n",
    "        trigram_list(tree,d,path_1,print_mode)\n",
    "        \n",
    "        counts = compute_counts_file(tokens)\n",
    "        counts_2d = compute_counts_2d_file_s(bigrams)\n",
    "        counts_3d = compute_counts_3d_file_s(trigrams)\n",
    "        unigram = compute_unigram_without_smoothing(counts)\n",
    "        bigram = compute_bigram_without_smoothing_s(counts_2d)\n",
    "        trigram = compute_trigram_without_smoothing_s(counts_3d)\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        c = []\n",
    "        d = []\n",
    "        \n",
    "        a_c = []\n",
    "        b_c = []\n",
    "        c_c = []\n",
    "        d_c = []\n",
    "        \n",
    "        for k in range(len(dense_bigram_1)):\n",
    "            k_1 = int(dense_bigram_1[k,0])\n",
    "            k_2 = int(dense_bigram_1[k,1])\n",
    "            \n",
    "            c.append(bigram[k_1,k_2])\n",
    "            c_c.append(counts_2d[k_1,k_2])\n",
    "            \n",
    "        for k in range(len(dense_trigram_1)):\n",
    "            k_1 = int(dense_trigram_1[k,0])\n",
    "            k_2 = int(dense_trigram_1[k,1])\n",
    "            k_3 = int(dense_trigram_1[k,2])\n",
    "            \n",
    "            d.append(trigram[k_1,k_2,k_3])\n",
    "            d_c.append(counts_3d[k_1,k_2,k_3])\n",
    "        \n",
    "        a = unigram\n",
    "        b = bigram.flatten()\n",
    "        c = np.array(c)\n",
    "        d = np.array(d)\n",
    "        \n",
    "        a_c = counts\n",
    "        b_c = counts_2d.flatten()\n",
    "        c_c = np.array(c_c)\n",
    "        d_c = np.array(d_c)\n",
    "        \n",
    "        if (data[i][\"is_buggy\"]==True):\n",
    "            a = np.append(a,1)\n",
    "            b = np.append(b,1)\n",
    "            c = np.append(c,1)\n",
    "            d = np.append(d,1)\n",
    "            \n",
    "            a_c = np.append(a_c,1)\n",
    "            b_c = np.append(b_c,1)\n",
    "            c_c = np.append(c_c,1)\n",
    "            d_c = np.append(d_c,1)\n",
    "        \n",
    "        else:\n",
    "            a = np.append(a,0)\n",
    "            b = np.append(b,0)\n",
    "            c = np.append(c,0)\n",
    "            d = np.append(d,0)\n",
    "            \n",
    "            a_c = np.append(a_c,0)\n",
    "            b_c = np.append(b_c,0)\n",
    "            c_c = np.append(c_c,0)\n",
    "            d_c = np.append(d_c,0)\n",
    "\n",
    "\n",
    "        unigram_data.append(a)\n",
    "        bigram_data.append(b)\n",
    "        dense_bigram_data.append(c)\n",
    "        dense_trigram_data.append(d)\n",
    "        \n",
    "        unigram_data_c.append(a_c)\n",
    "        bigram_data_c.append(b_c)\n",
    "        dense_bigram_data_c.append(c_c)\n",
    "        dense_trigram_data_c.append(d_c)\n",
    "                \n",
    "unigram_data = np.array(unigram_data)\n",
    "dense_bigram_data = np.array(dense_bigram_data)\n",
    "dense_trigram_data = np.array(dense_trigram_data)\n",
    "\n",
    "unigram_data_c = np.array(unigram_data_c)\n",
    "dense_bigram_data_c = np.array(dense_bigram_data_c)\n",
    "dense_trigram_data_c = np.array(dense_trigram_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "n = len(words)\n",
    "s_unigram_data = []\n",
    "s_bigram_data = []\n",
    "s_dense_bigram_data = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if True:\n",
    "        print(i)\n",
    "        counts = np.zeros((n))\n",
    "        counts_2d = np.zeros((n,n))\n",
    "        file_text = data[i][\"codes\"]\n",
    "        tree = parse_m(file_text)\n",
    "        tokens = token_list_2(tree)\n",
    "        d = 0\n",
    "        print_mode = 0\n",
    "        path_1 = []\n",
    "        bigrams = []\n",
    "        bigram_list(tree,d,path_1,print_mode)\n",
    "        counts = compute_counts_file(tokens)\n",
    "        counts_2d = compute_counts_2d_file_s(bigrams)\n",
    "        unigram = compute_unigram_without_smoothing(counts)\n",
    "        bigram = compute_bigram_without_smoothing_s(counts_2d)\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        c = []\n",
    "        for k in range(len(s_dense_bigram_1)):\n",
    "            k_1 = int(s_dense_bigram_1[k,0])\n",
    "            k_2 = int(s_dense_bigram_1[k,1])\n",
    "            c.append(bigram[k_1,k_2])\n",
    "           \n",
    "        a = unigram\n",
    "        b = bigram.flatten()\n",
    "        c = np.array(c)\n",
    "        \n",
    "        if (data[i][\"is_buggy\"]==True):\n",
    "            a = np.append(a,1)\n",
    "            b = np.append(b,1)\n",
    "            c = np.append(c,1)\n",
    "        else:\n",
    "            a = np.append(a,0)\n",
    "            b = np.append(b,0)\n",
    "            c = np.append(c,0)\n",
    "\n",
    "        s_unigram_data.append(a)\n",
    "        s_bigram_data.append(b)\n",
    "        s_dense_bigram_data.append(c)\n",
    "                \n",
    "s_unigram_data = np.array(s_unigram_data)\n",
    "s_dense_bigram_data = np.array(s_dense_bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST4)\n",
    "np.save('./data_2/unigram_data_AST4.npy',unigram_data)\n",
    "np.save('./data_2/dense_bigram_data_AST4.npy',dense_bigram_data)\n",
    "np.save('./data_2/dense_trigram_data_AST4.npy',dense_trigram_data)\n",
    "\n",
    "np.save('./data_2/unigram_data_c_AST4.npy',unigram_data_c)\n",
    "np.save('./data_2/dense_bigram_data_c_AST4.npy',dense_bigram_data_c)\n",
    "np.save('./data_2/dense_trigram_data_c_AST4.npy',dense_trigram_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "np.save('./data_2/s_unigram_data_m.npy',s_unigram_data)\n",
    "np.save('./data_2/s_dense_bigram_data_m.npy',s_dense_bigram_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST4)\n",
    "unigram_data = np.load('./data_2/unigram_data_AST4.npy')\n",
    "dense_bigram_data = np.load('./data_2/dense_bigram_data_AST4.npy')\n",
    "dense_trigram_data = np.load('./data_2/dense_trigram_data_AST4.npy')\n",
    "\n",
    "unigram_data_c = np.load('./data_2/unigram_data_c_AST4.npy')\n",
    "dense_bigram_data_c = np.load('./data_2/dense_bigram_data_c_AST4.npy')\n",
    "dense_trigram_data_c = np.load('./data_2/dense_trigram_data_c_AST4.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# methods\n",
    "s_unigram_data = np.load('./data_2/s_unigram_data_m.npy')\n",
    "s_dense_bigram_data = np.load('./data_2/s_dense_bigram_data_m.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing ngrams probability & count for each text file (second method path-based AST):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(AST_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ngram_type)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_counts_file_s_bigrams(b):\n",
    "    counts_bigram = np.zeros((n))\n",
    "    counts_2d_bigram = np.zeros((n,n))\n",
    "    for i in range(len(b)):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (b[i][0] == words[j]):\n",
    "                i_1 = j\n",
    "            if (b[i][1] == words[j]):\n",
    "                i_2 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        \n",
    "        counts_bigram[i_1] += 1\n",
    "        counts_2d_bigram[i_1,i_2] += 1\n",
    "        \n",
    "    return counts_2d_bigram,counts_bigram\n",
    "\n",
    "def compute_counts_file_s_trigrams(t):\n",
    "    counts_2d_trigram = np.zeros((n,n))\n",
    "    counts_3d_trigram = np.zeros((n,n,n))\n",
    "    for i in range(len(t)):\n",
    "        i_1 = -1\n",
    "        i_2 = -1\n",
    "        i_3 = -1\n",
    "        for j in range(len(words)):\n",
    "            if (t[i][0] == words[j]):\n",
    "                i_1 = j\n",
    "            if (t[i][1] == words[j]):\n",
    "                i_2 = j\n",
    "            if (t[i][2] == words[j]):\n",
    "                i_3 = j\n",
    "        if (i_1 == -1):\n",
    "            i_1 = n-1\n",
    "        if (i_2 == -1):\n",
    "            i_2 = n-1\n",
    "        if (i_3 == -1):\n",
    "            i_3 = n-1\n",
    "            \n",
    "        counts_2d_trigram[i_1,i_2] += 1\n",
    "        counts_3d_trigram[i_1,i_2,i_3] += 1\n",
    "        \n",
    "    return counts_3d_trigram,counts_2d_trigram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "n = len(words)\n",
    "unigram_data = []\n",
    "bigram_data = []\n",
    "dense_bigram_data = []\n",
    "dense_trigram_data = []\n",
    "\n",
    "unigram_data_c = []\n",
    "bigram_data_c = []\n",
    "dense_bigram_data_c = []\n",
    "dense_trigram_data_c = []\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if True:\n",
    "        print(i)\n",
    "        counts_unigram = np.zeros((n))\n",
    "        counts_bigram = np.zeros((n))\n",
    "        counts_2d_bigram = np.zeros((n,n))\n",
    "        counts_2d_trigram = np.zeros((n,n))\n",
    "        counts_3d_trigram = np.zeros((n,n,n))\n",
    "        file_text = data[i][\"codes\"]\n",
    "        tree = javalang.parse.parse(file_text)\n",
    "        tokens = token_list_2(tree)\n",
    "        d = 0\n",
    "        print_mode = 0\n",
    "        path_1 = []\n",
    "        bigrams = []\n",
    "        bigram_list(tree,d,path_1,print_mode)\n",
    "        \n",
    "        d = 0\n",
    "        path_1 = []\n",
    "        trigrams = []\n",
    "        print_mode = 0\n",
    "        trigram_list(tree,d,path_1,print_mode)\n",
    "        \n",
    "        counts_unigram = compute_counts_file(tokens)\n",
    "        \n",
    "        counts_2d_bigram,counts_bigram = compute_counts_file_s_bigrams(bigrams)\n",
    "        \n",
    "        counts_3d_trigram,counts_2d_trigram = compute_counts_file_s_trigrams(trigrams)\n",
    "        \n",
    "        unigram = compute_unigram_without_smoothing(counts_unigram)\n",
    "        bigram = compute_bigram_without_smoothing(counts_2d_bigram,counts_bigram)\n",
    "        trigram = compute_trigram_without_smoothing(counts_3d_trigram,counts_2d_trigram)\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        c = []\n",
    "        d = []\n",
    "        \n",
    "        a_c = []\n",
    "        b_c = []\n",
    "        c_c = []\n",
    "        d_c = []\n",
    "        \n",
    "        for k in range(len(dense_bigram_1)):\n",
    "            k_1 = int(dense_bigram_1[k,0])\n",
    "            k_2 = int(dense_bigram_1[k,1])\n",
    "            \n",
    "            c.append(bigram[k_1,k_2])\n",
    "            c_c.append(counts_2d_bigram[k_1,k_2])\n",
    "            \n",
    "        for k in range(len(dense_trigram_1)):\n",
    "            k_1 = int(dense_trigram_1[k,0])\n",
    "            k_2 = int(dense_trigram_1[k,1])\n",
    "            k_3 = int(dense_trigram_1[k,2])\n",
    "            \n",
    "            d.append(trigram[k_1,k_2,k_3])\n",
    "            d_c.append(counts_3d_trigram[k_1,k_2,k_3])\n",
    "        \n",
    "        a = unigram\n",
    "        b = bigram.flatten()\n",
    "        c = np.array(c)\n",
    "        d = np.array(d)\n",
    "        \n",
    "        a_c = counts\n",
    "        b_c = counts_2d.flatten()\n",
    "        c_c = np.array(c_c)\n",
    "        d_c = np.array(d_c)\n",
    "        \n",
    "        if (data[i][\"is_buggy\"]==True):\n",
    "            a = np.append(a,1)\n",
    "            b = np.append(b,1)\n",
    "            c = np.append(c,1)\n",
    "            d = np.append(d,1)\n",
    "            \n",
    "            a_c = np.append(a_c,1)\n",
    "            b_c = np.append(b_c,1)\n",
    "            c_c = np.append(c_c,1)\n",
    "            d_c = np.append(d_c,1)\n",
    "        \n",
    "        else:\n",
    "            a = np.append(a,0)\n",
    "            b = np.append(b,0)\n",
    "            c = np.append(c,0)\n",
    "            d = np.append(d,0)\n",
    "            \n",
    "            a_c = np.append(a_c,0)\n",
    "            b_c = np.append(b_c,0)\n",
    "            c_c = np.append(c_c,0)\n",
    "            d_c = np.append(d_c,0)\n",
    "\n",
    "\n",
    "        unigram_data.append(a)\n",
    "        bigram_data.append(b)\n",
    "        dense_bigram_data.append(c)\n",
    "        dense_trigram_data.append(d)\n",
    "        \n",
    "        unigram_data_c.append(a_c)\n",
    "        bigram_data_c.append(b_c)\n",
    "        dense_bigram_data_c.append(c_c)\n",
    "        dense_trigram_data_c.append(d_c)\n",
    "                \n",
    "unigram_data = np.array(unigram_data)\n",
    "dense_bigram_data = np.array(dense_bigram_data)\n",
    "dense_trigram_data = np.array(dense_trigram_data)\n",
    "\n",
    "unigram_data_c = np.array(unigram_data_c)\n",
    "dense_bigram_data_c = np.array(dense_bigram_data_c)\n",
    "dense_trigram_data_c = np.array(dense_trigram_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Time\n",
    "# files\n",
    "n = len(words)\n",
    "unigram_data = []\n",
    "bigram_data = []\n",
    "dense_bigram_data = []\n",
    "dense_trigram_data = []\n",
    "\n",
    "unigram_data_c = []\n",
    "bigram_data_c = []\n",
    "dense_bigram_data_c = []\n",
    "dense_trigram_data_c = []\n",
    "\n",
    "bigram_computing_time = 0\n",
    "trigram_computing_time = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if True:\n",
    "        print(i)\n",
    "        counts_unigram = np.zeros((n))\n",
    "        counts_bigram = np.zeros((n))\n",
    "        counts_2d_bigram = np.zeros((n,n))\n",
    "        counts_2d_trigram = np.zeros((n,n))\n",
    "        counts_3d_trigram = np.zeros((n,n,n))\n",
    "        file_text = data[i][\"codes\"]\n",
    "        tree = javalang.parse.parse(file_text)\n",
    "        tokens = token_list_2(tree)\n",
    "        d = 0\n",
    "        print_mode = 0\n",
    "        path_1 = []\n",
    "        bigrams = []\n",
    "        bigram_list(tree,d,path_1,print_mode)\n",
    "        \n",
    "        d = 0\n",
    "        path_1 = []\n",
    "        trigrams = []\n",
    "        print_mode = 0\n",
    "        trigram_list(tree,d,path_1,print_mode)\n",
    "        \n",
    "        counts_unigram = compute_counts_file(tokens)\n",
    "        counts_2d_bigram,counts_bigram = compute_counts_file_s_bigrams(bigrams)\n",
    "        counts_3d_trigram,counts_2d_trigram = compute_counts_file_s_trigrams(trigrams)\n",
    "        \n",
    "        unigram = compute_unigram_without_smoothing(counts_unigram)\n",
    "        \n",
    "        b_1_t = time.time()\n",
    "        bigram = compute_bigram_without_smoothing_d(counts_2d_bigram,counts_bigram)\n",
    "        b_2_t = time.time()\n",
    "        bigram_computing_time += (b_2_t -b_1_t)\n",
    "        \n",
    "        t_1_t = time.time()\n",
    "        trigram = compute_trigram_without_smoothing_d(counts_3d_trigram,counts_2d_trigram)\n",
    "        t_2_t = time.time()\n",
    "        trigram_computing_time += (t_2_t -t_1_t)\n",
    "        \n",
    "        a = []\n",
    "        b = []\n",
    "        c = []\n",
    "        d = []\n",
    "        \n",
    "        a_c = []\n",
    "        b_c = []\n",
    "        c_c = []\n",
    "        d_c = []\n",
    "        \n",
    "        for k in range(len(dense_bigram_1)):\n",
    "            k_1 = int(dense_bigram_1[k,0])\n",
    "            k_2 = int(dense_bigram_1[k,1])\n",
    "            \n",
    "            c.append(bigram[k_1,k_2])\n",
    "            c_c.append(counts_2d_bigram[k_1,k_2])\n",
    "            \n",
    "        for k in range(len(dense_trigram_1)):\n",
    "            k_1 = int(dense_trigram_1[k,0])\n",
    "            k_2 = int(dense_trigram_1[k,1])\n",
    "            k_3 = int(dense_trigram_1[k,2])\n",
    "            \n",
    "            d.append(trigram[k_1,k_2,k_3])\n",
    "            d_c.append(counts_3d_trigram[k_1,k_2,k_3])\n",
    "        \n",
    "        a = unigram\n",
    "        b = bigram.flatten()\n",
    "        c = np.array(c)\n",
    "        d = np.array(d)\n",
    "        \n",
    "        a_c = counts\n",
    "        b_c = counts_2d.flatten()\n",
    "        c_c = np.array(c_c)\n",
    "        d_c = np.array(d_c)\n",
    "        \n",
    "        if (data[i][\"is_buggy\"]==True):\n",
    "            a = np.append(a,1)\n",
    "            b = np.append(b,1)\n",
    "            c = np.append(c,1)\n",
    "            d = np.append(d,1)\n",
    "            \n",
    "            a_c = np.append(a_c,1)\n",
    "            b_c = np.append(b_c,1)\n",
    "            c_c = np.append(c_c,1)\n",
    "            d_c = np.append(d_c,1)\n",
    "        \n",
    "        else:\n",
    "            a = np.append(a,0)\n",
    "            b = np.append(b,0)\n",
    "            c = np.append(c,0)\n",
    "            d = np.append(d,0)\n",
    "            \n",
    "            a_c = np.append(a_c,0)\n",
    "            b_c = np.append(b_c,0)\n",
    "            c_c = np.append(c_c,0)\n",
    "            d_c = np.append(d_c,0)\n",
    "\n",
    "\n",
    "        unigram_data.append(a)\n",
    "        bigram_data.append(b)\n",
    "        dense_bigram_data.append(c)\n",
    "        dense_trigram_data.append(d)\n",
    "        \n",
    "        unigram_data_c.append(a_c)\n",
    "        bigram_data_c.append(b_c)\n",
    "        dense_bigram_data_c.append(c_c)\n",
    "        dense_trigram_data_c.append(d_c)\n",
    "                \n",
    "unigram_data = np.array(unigram_data)\n",
    "dense_bigram_data = np.array(dense_bigram_data)\n",
    "dense_trigram_data = np.array(dense_trigram_data)\n",
    "\n",
    "unigram_data_c = np.array(unigram_data_c)\n",
    "dense_bigram_data_c = np.array(dense_bigram_data_c)\n",
    "dense_trigram_data_c = np.array(dense_trigram_data_c)\n",
    "\n",
    "print('bigram computing time:',round(bigram_computing_time,4))\n",
    "print('trigram computing time:',round(trigram_computing_time,4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST4)\n",
    "np.save('./data_2/unigram_data_AST4.npy',unigram_data)\n",
    "np.save('./data_2/dense_bigram_data_AST4.npy',dense_bigram_data)\n",
    "np.save('./data_2/dense_trigram_data_AST4.npy',dense_trigram_data)\n",
    "\n",
    "np.save('./data_2/unigram_data_c_AST4.npy',unigram_data_c)\n",
    "np.save('./data_2/dense_bigram_data_c_AST4.npy',dense_bigram_data_c)\n",
    "np.save('./data_2/dense_trigram_data_c_AST4.npy',dense_trigram_data_c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files (AST4)\n",
    "unigram_data = np.load('./data_2/unigram_data_AST4.npy')\n",
    "dense_bigram_data = np.load('./data_2/dense_bigram_data_AST4.npy')\n",
    "dense_trigram_data = np.load('./data_2/dense_trigram_data_AST4.npy')\n",
    "\n",
    "unigram_data_c = np.load('./data_2/unigram_data_c_AST4.npy')\n",
    "dense_bigram_data_c = np.load('./data_2/dense_bigram_data_c_AST4.npy')\n",
    "dense_trigram_data_c = np.load('./data_2/dense_trigram_data_c_AST4.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting CK metrics for each text file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# files\n",
    "ck_metrics_data = []\n",
    "y = 0\n",
    "n = 0\n",
    "\n",
    "for i in range(len(data)):\n",
    "    if True:\n",
    "        print(i)\n",
    "        a = data[i][\"ck_metrics\"]\n",
    "        a = [float(j) for j in a]\n",
    "        \n",
    "        if (data[i][\"is_buggy\"]==True):\n",
    "            a.append(1)\n",
    "            y += 1\n",
    "        else:\n",
    "            a.append(0)\n",
    "            n += 1\n",
    "            \n",
    "        ck_metrics_data.append(a)\n",
    "        \n",
    "ck_metrics_data = np.array(ck_metrics_data)\n",
    "\n",
    "print(y)\n",
    "print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ck_metrics_data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data_2/ck_metrics_data.npy',ck_metrics_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ck_metrics_data = np.load('./data_2/ck_metrics_data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Defining a method to calculate metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(Y_test,predicted_result):\n",
    "    c_matrix = confusion_matrix(Y_test,predicted_result)\n",
    "    print('confusion matrix:')\n",
    "    print(c_matrix)\n",
    "    tn = c_matrix[0,0]\n",
    "    fp = c_matrix[0,1]\n",
    "    fn = c_matrix[1,0]\n",
    "    tp = c_matrix[1,1]\n",
    "    \n",
    "    if ((tp + fp) == 0):\n",
    "        precision = 'not_defined'\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    if ((tp + fn) == 0):\n",
    "        recall = 'not_defined'\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "    \n",
    "    accuracy = (tp + tn)/(tn + fp + fn + tp)\n",
    "    \n",
    "    if (precision == 'not_defined' or recall == 'not_defined' or (precision + recall) == 0):\n",
    "        f1_score = 'not_defined'\n",
    "    else:\n",
    "        f1_score = (2 * precision * recall)/(precision + recall)\n",
    "        \n",
    "        \n",
    "    if ((fp + tn) == 0):\n",
    "        fp_rate = 'not_defined'\n",
    "    else:\n",
    "        fp_rate = fp / (fp + tn)\n",
    "\n",
    "    print('precision:',precision)\n",
    "    print('recall:',recall)\n",
    "    print('accuracy:',accuracy)\n",
    "    print('f1 score:',f1_score)\n",
    "    print('FP rate:',fp_rate)\n",
    "    return (precision,recall,accuracy,f1_score,fp_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plotting the training accuracy and loss:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_variables(history):\n",
    "    training_accuracy = history.history['acc']\n",
    "    validation_accuracy = history.history['val_acc']\n",
    "    training_loss = history.history['loss']\n",
    "    validation_loss = history.history['val_loss']\n",
    "    epochs = range(1,(epochs_num+1))\n",
    "\n",
    "    plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    "    plt.plot(epochs,training_accuracy,color='darkblue',label='training accuracy')\n",
    "    plt.plot(epochs,validation_accuracy,color='red',label='validation accuracy')\n",
    "    plt.title('Diagram of training accuracy and epochs',fontsize=16)\n",
    "    plt.xlabel('epochs',fontsize=14)\n",
    "    plt.ylabel('training accuracy',fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()\n",
    "\n",
    "    plt.plot(epochs,training_loss,color='darkblue',label='training loss')\n",
    "    plt.plot(epochs,validation_loss,color='red',label='validation loss')\n",
    "    plt.title('Diagram of training loss and epochs',fontsize=16)\n",
    "    plt.xlabel('epochs',fontsize=14)\n",
    "    plt.ylabel('training loss',fontsize=14)\n",
    "    plt.legend(fontsize=12)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concatenating two datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_1(data_1,data_2):\n",
    "    length_1 = data_1.shape[1]-1\n",
    "    feature_1 = data_1[:,0:length_1]\n",
    "    feature = np.concatenate((feature_1,data_2),axis=1)\n",
    "    return feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_num = 8\n",
    "def convolutional(data,mode):\n",
    "    n_data = data.shape[0]\n",
    "    length = data.shape[1] - 1\n",
    "    c_result = data[:,length]\n",
    "    feature = data[:,0:length]\n",
    "    print(feature.shape)\n",
    "    feature = feature.reshape(feature.shape[0],feature.shape[1],1)\n",
    "    print(feature.shape)\n",
    "\n",
    "    X_train, X_test, Y_train, Y_test = train_test_split(feature, c_result, test_size=0.1)\n",
    "\n",
    "    if (mode == 1):\n",
    "        model = tf.keras.Sequential()\n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(length,1)))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(MaxPooling1D())\n",
    "        \n",
    "        model.add(Conv1D(filters=64, kernel_size=3, activation='relu'))\n",
    "        model.add(Dense(16, activation='relu'))\n",
    "        model.add(MaxPooling1D())\n",
    "        \n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(2, activation='softmax'))\n",
    "\n",
    "    print(model.summary())  \n",
    "    model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "    history = model.fit(X_train, Y_train, batch_size=512, epochs=epochs_num, validation_data=(X_test,Y_test))\n",
    "\n",
    "    print('\\nEvaluation:')\n",
    "    model.evaluate(X_test,Y_test)\n",
    "    \n",
    "    predicted_r = model.predict(X_test)\n",
    "    predicted_result = np.argmax(predicted_r,axis=1)\n",
    "    Y_test = Y_test.astype(int)\n",
    "    computed_metrics = compute_metrics(Y_test,predicted_result)\n",
    "\n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "history = convolutional(unigram_data,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_variables(history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dividing the data into fixed sets of test and train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data_2(data):\n",
    "    l_1 =data.shape[0]\n",
    "    l_2 = int(np.ceil(l_1 / 10))\n",
    "    random.seed(10)\n",
    "    index_list = random.sample(range(l_1),l_2)\n",
    "    test_data = []\n",
    "    train_data = []\n",
    "    for i in range(l_1):\n",
    "        if i in index_list:\n",
    "            test_data.append(data[i])\n",
    "        else:\n",
    "            train_data.append(data[i])\n",
    "    test_data = np.array(test_data)\n",
    "    train_data = np.array(train_data)\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_1, test_1 = split_data_2(unigram_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_roc(fp_r,tp_r):\n",
    "    plt.rcParams[\"figure.figsize\"] = (3,3)\n",
    "    plt.plot(fp_r,tp_r)\n",
    "    plt.xlabel('FP rate')\n",
    "    plt.ylabel('TP rate')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Fold splittig the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kf_data_split(data):\n",
    "    kf_data = []\n",
    "    kf_1 = KFold(n_splits = 10)\n",
    "    for train_i, test_i in kf_1.split(data):\n",
    "        train = data[train_i]\n",
    "        test = data[test_i]\n",
    "        kf_data.append([train,test])\n",
    "    return(kf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic regression with specified threshold and K-Fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_3(train_1,test_1):\n",
    "    max_iter = 1000000\n",
    "    \n",
    "#     threshold = 0.1623\n",
    "    \n",
    "    length = train_1.shape[1] - 1\n",
    "    X_train = train_1[:,0:length]\n",
    "    Y_train = train_1[:,length]\n",
    "    X_test = test_1[:,0:length]\n",
    "    Y_test = test_1[:,length]\n",
    "    \n",
    "    logistic_regression = LogisticRegression()\n",
    "    model = logistic_regression.fit(X_train, Y_train)\n",
    "    \n",
    "    predicted_r = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    fp_r, tp_r, t_array = roc_curve(Y_test,predicted_r)\n",
    "        \n",
    "    plot_roc(fp_r,tp_r)\n",
    "    \n",
    "    auc_1 = roc_auc_score(Y_test,predicted_r)\n",
    "    print('AUC:',auc_1)\n",
    "    \n",
    "    print('threshold:',threshold)\n",
    "    length_2 = int(predicted_r.shape[0])\n",
    "    \n",
    "    predicted_result = np.zeros(length_2)\n",
    "    for i in range(length_2):\n",
    "        if (predicted_r[i] >= threshold):\n",
    "            predicted_result[i] = 1\n",
    "        else:\n",
    "            predicted_result[i] = 0\n",
    "    \n",
    "    Y_test = Y_test.astype(int)\n",
    "    computed_metrics = compute_metrics(Y_test,predicted_result)\n",
    "    return computed_metrics,auc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest with specified threshold and K-Fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classifier_3(train_1,test_1):\n",
    "#     threshold = 0.1623\n",
    "    length = train_1.shape[1] - 1\n",
    "    X_train = train_1[:,0:length]\n",
    "    Y_train = train_1[:,length]\n",
    "    X_test = test_1[:,0:length]\n",
    "    Y_test = test_1[:,length]\n",
    "    \n",
    "    clf = RandomForestClassifier(max_depth=m_depth,random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    predicted_r = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    fp_r, tp_r, t_array = roc_curve(Y_test,predicted_r)\n",
    "        \n",
    "    plot_roc(fp_r,tp_r)\n",
    "    \n",
    "    auc_1 = roc_auc_score(Y_test,predicted_r)\n",
    "    print('AUC:',auc_1)\n",
    "    \n",
    "    print('threshold:',threshold)\n",
    "    length_2 = int(predicted_r.shape[0])\n",
    "    \n",
    "    predicted_result = np.zeros(length_2)\n",
    "    for i in range(length_2):\n",
    "        if (predicted_r[i] >= threshold):\n",
    "            predicted_result[i] = 1\n",
    "        else:\n",
    "            predicted_result[i] = 0\n",
    "    \n",
    "    Y_test = Y_test.astype(int)\n",
    "    computed_metrics = compute_metrics(Y_test,predicted_result)\n",
    "    return computed_metrics,auc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sequential NN with specified threshold and K-Fold cross-validation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_num = 100\n",
    "def sequential_3(train_1,test_1,mode):\n",
    "#     threshold = 0.2052\n",
    "    \n",
    "    length = train_1.shape[1] - 1\n",
    "    Y_train = train_1[:,length]\n",
    "    Y_test = test_1[:,length]\n",
    "    \n",
    "    X_train = train_1[:,0:length]\n",
    "    X_test = test_1[:,0:length]\n",
    "    \n",
    "    if (mode == 1):\n",
    "        batch_size_1 = 8\n",
    "        model = keras.Sequential([keras.layers.Dense(64, input_shape=(length,), activation='relu'), \n",
    "                                  keras.layers.Dropout(0.3), \n",
    "                                  keras.layers.Dense(32, activation='relu'), \n",
    "                                  keras.layers.Dropout(0.2), \n",
    "                                  keras.layers.Dense(8, activation='relu'),\n",
    "                                  (keras.layers.Dense(1, activation='sigmoid'))])\n",
    "    if (mode == 2):\n",
    "        batch_size_1 = 8\n",
    "        model = keras.Sequential([keras.layers.Dense(128, input_shape=(length,), activation='relu'), \n",
    "                                  keras.layers.Dropout(0.3), \n",
    "                                  keras.layers.Dense(32, activation='relu'), \n",
    "                                  keras.layers.Dropout(0.2), \n",
    "                                  keras.layers.Dense(16, activation='relu'),\n",
    "                                  (keras.layers.Dense(1, activation='sigmoid'))])\n",
    "    if (mode == 3):\n",
    "        batch_size_1 = 8\n",
    "        model = keras.Sequential([keras.layers.Dense(32, input_shape=(length,), activation='relu'), \n",
    "                                  keras.layers.Dropout(0.3), \n",
    "                                  keras.layers.Dense(8, activation='relu'), \n",
    "                                  (keras.layers.Dense(1, activation='sigmoid'))])\n",
    "    if (mode == 4):\n",
    "        batch_size_1 = 8\n",
    "        model = keras.Sequential([keras.layers.Dense(128, input_shape=(length,), activation='relu'), \n",
    "                                  keras.layers.Dropout(0.3), \n",
    "                                  keras.layers.Dense(32, activation='relu'), \n",
    "                                  keras.layers.Dropout(0.2), \n",
    "                                  keras.layers.Dense(16, activation='relu'),\n",
    "                                  (keras.layers.Dense(1, activation='sigmoid'))])\n",
    "    if (mode == 5):\n",
    "        batch_size_1 = 8\n",
    "        model = keras.Sequential([keras.layers.Dense(256, input_shape=(length,), activation='relu'), \n",
    "                                  keras.layers.Dropout(0.3), \n",
    "                                  keras.layers.Dense(64, activation='relu'), \n",
    "                                  keras.layers.Dropout(0.2), \n",
    "                                  keras.layers.Dense(16, activation='relu'),\n",
    "                                  (keras.layers.Dense(1, activation='sigmoid'))])\n",
    "    if (mode == 6):\n",
    "        batch_size_1 = 8\n",
    "        model = keras.Sequential([keras.layers.Dense(16, input_shape=(length,), activation='relu'), \n",
    "                                  keras.layers.Dropout(0.3), \n",
    "                                  keras.layers.Dense(4, activation='relu'),\n",
    "                                  (keras.layers.Dense(1, activation='sigmoid'))])\n",
    "\n",
    "    model.compile(optimizer='adam', loss=keras.losses.BinaryCrossentropy(from_logits=False), metrics=['accuracy'])\n",
    "\n",
    "    history = model.fit(X_train, Y_train, batch_size=batch_size_1, epochs=epochs_num, validation_data=(X_test,Y_test))\n",
    "\n",
    "    print('\\nEvaluation:')\n",
    "    model.evaluate(X_test,Y_test)\n",
    "    \n",
    "    predicted_r = model.predict(X_test)\n",
    "    \n",
    "    fp_r, tp_r, t_array = roc_curve(Y_test,predicted_r)\n",
    "    \n",
    "    plot_roc(fp_r,tp_r)\n",
    "    \n",
    "    auc_1 = roc_auc_score(Y_test,predicted_r)\n",
    "    print('AUC:',auc_1)\n",
    "    \n",
    "    print('threshold:',threshold)\n",
    "    \n",
    "    length_2 = int(predicted_r.shape[0])\n",
    "    \n",
    "    predicted_result = np.zeros(length_2)\n",
    "    for i in range(length_2):\n",
    "        if (predicted_r[i][0] >= threshold):\n",
    "            predicted_result[i] = 1\n",
    "        else:\n",
    "            predicted_result[i] = 0\n",
    "    \n",
    "    Y_test = Y_test.astype(int)\n",
    "    computed_metrics = compute_metrics(Y_test,predicted_result)\n",
    "    \n",
    "    return computed_metrics,auc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-nearest neighbors classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KN_classifier_3(train_1,test_1):\n",
    "#     threshold = 0.1623\n",
    "    length = train_1.shape[1] - 1\n",
    "    X_train = train_1[:,0:length]\n",
    "    Y_train = train_1[:,length]\n",
    "    X_test = test_1[:,0:length]\n",
    "    Y_test = test_1[:,length]\n",
    "    \n",
    "    k_value = math.floor(math.sqrt(train_1.shape[0]))\n",
    "    if (k_value % 2 == 0):\n",
    "        print('e')\n",
    "        k_value += 1\n",
    "    else:\n",
    "        print('o')\n",
    "    print(k_value)\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = k_value)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    predicted_r = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    fp_r, tp_r, t_array = roc_curve(Y_test,predicted_r)\n",
    "        \n",
    "    plot_roc(fp_r,tp_r)\n",
    "    \n",
    "    auc_1 = roc_auc_score(Y_test,predicted_r)\n",
    "    print('AUC:',auc_1)\n",
    "    \n",
    "    print('threshold:',threshold)\n",
    "    length_2 = int(predicted_r.shape[0])\n",
    "    \n",
    "    predicted_result = np.zeros(length_2)\n",
    "    for i in range(length_2):\n",
    "        if (predicted_r[i] >= threshold):\n",
    "            predicted_result[i] = 1\n",
    "        else:\n",
    "            predicted_result[i] = 0\n",
    "    \n",
    "    Y_test = Y_test.astype(int)\n",
    "    computed_metrics = compute_metrics(Y_test,predicted_result)\n",
    "    return computed_metrics,auc_1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Computing average of metrics using K-Fold cross-validation (probabilistic classification):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression => m:1\n",
    "# Sequential NN => m:3,4\n",
    "def kf_average(d_1,m):\n",
    "    trials_n = 1\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "    accuracy_array = []\n",
    "    f1_score_array = []\n",
    "    fp_rate_array = []\n",
    "    auc_array = []\n",
    "    data = kf_data_split(d_1)\n",
    "    for t in range(trials_n):\n",
    "        print('\\nTrial number:',t+1)\n",
    "        for i in range(len(data)):\n",
    "            print('\\nIteration number:',(i+1))\n",
    "            train_data = data[i][0]\n",
    "            test_data = data[i][1]\n",
    "\n",
    "            if (m == 1):\n",
    "                result,auc_result = logistic_regression_3(train_data,test_data)\n",
    "            if (m == 2):\n",
    "                result,auc_result = RF_classifier_3(train_data,test_data)\n",
    "            elif (m == 3):\n",
    "                result,auc_result = sequential_3(train_data,test_data,1)\n",
    "            elif (m == 4):\n",
    "                result,auc_result = sequential_3(train_data,test_data,2)\n",
    "            elif (m == 5):\n",
    "                result,auc_result = sequential_3(train_data,test_data,3)\n",
    "            elif (m == 6):\n",
    "                result,auc_result = sequential_3(train_data,test_data,4)\n",
    "            elif (m == 7):\n",
    "                result,auc_result = sequential_3(train_data,test_data,5)\n",
    "            elif (m == 8):\n",
    "                result,auc_result = sequential_3(train_data,test_data,6)\n",
    "            elif (m == 9):\n",
    "                result,auc_result = KN_classifier_3(train_data,test_data)\n",
    "                \n",
    "            precision,recall,accuracy,f1_score,fp_rate = result\n",
    "\n",
    "            if (precision != 'not_defined'):\n",
    "                precision_array.append(precision)\n",
    "            if (recall != 'not_defined'):\n",
    "                recall_array.append(recall)\n",
    "            if (accuracy != 'not_defined'):\n",
    "                accuracy_array.append(accuracy)\n",
    "            if (f1_score != 'not_defined'):\n",
    "                f1_score_array.append(f1_score)\n",
    "            if (fp_rate != 'not_defined'):\n",
    "                fp_rate_array.append(fp_rate)\n",
    "            if (auc_result != 'not_defined'):\n",
    "                auc_array.append(auc_result)\n",
    "    \n",
    "    precision_array = np.array(precision_array)\n",
    "    recall_array = np.array(recall_array)\n",
    "    accuracy_array = np.array(accuracy_array)\n",
    "    f1_score_array = np.array(f1_score_array)\n",
    "    fp_rate_array = np.array(fp_rate_array)\n",
    "    auc_array = np.array(auc_array)\n",
    "    \n",
    "    print('\\nprecision array:',precision_array,'\\naverage precision:',precision_array.mean(),'\\n')\n",
    "    print('\\nrecall array:',recall_array,'\\naverage recall:',recall_array.mean(),'\\n')\n",
    "    print('\\naccuracy array:',accuracy_array,'\\naverage accuracy:',accuracy_array.mean(),'\\n')\n",
    "    print('\\nf1_score array:',f1_score_array,'\\naverage f1_score:',f1_score_array.mean(),'\\n')\n",
    "    print('\\nfp_rate array:',fp_rate_array,'\\naverage fp_rate:',fp_rate_array.mean(),'\\n')\n",
    "    print('\\nauc array:',auc_array,'\\naverage auc:',auc_array.mean(),'\\n')\n",
    "    \n",
    "    print('precision:',round(precision_array.mean(),4))\n",
    "    print('recall:',round(recall_array.mean(),4))\n",
    "    print('accuracy:',round(accuracy_array.mean(),4))\n",
    "    print('f1 score:',round(f1_score_array.mean(),4))\n",
    "    print('fp rate:',round(fp_rate_array.mean(),4))\n",
    "    print('auc:',round(auc_array.mean(),4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start\n",
    "kf_average(b_data,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_1(data,k_value,score_f):\n",
    "    length = data.shape[1] - 1\n",
    "    X = data[:,0:length]\n",
    "    y = data[:,length].reshape(-1,1)\n",
    "    if (score_f == 1):\n",
    "        s = SelectKBest(score_func=f_classif, k=k_value)\n",
    "    elif (score_f == 2):\n",
    "        s = SelectKBest(score_func=chi2, k=k_value)\n",
    "    elif (score_f == 3):\n",
    "        s = SelectKBest(score_func=mutual_info_classif, k=k_value)\n",
    "    elif (score_f == 4):\n",
    "        s = SelectFpr(score_func=chi2, alpha=0.01)\n",
    "    elif (score_f == 5):\n",
    "        s = SelectFdr(score_func=chi2, alpha=0.01)\n",
    "        \n",
    "    new_X = s.fit_transform(X,y)\n",
    "    columns = s.get_support(indices=True)\n",
    "    scores = s.scores_[s.get_support()]\n",
    "    new_data = np.hstack((new_X,y))\n",
    "    \n",
    "    zipped_f = zip(scores,columns)\n",
    "    zipped_f = sorted(zipped_f,reverse=True)\n",
    "    sorted_columns = [column for (score,column) in zipped_f]\n",
    "    \n",
    "    return new_data,sorted_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_features_2(data,k_value,score_f):\n",
    "    length = data.shape[1] - 1\n",
    "    X = data[:,0:length]\n",
    "    y = data[:,length].reshape(-1,1)\n",
    "    \n",
    "    if (score_f <= 5):\n",
    "        if (score_f == 1):\n",
    "            s = SelectKBest(score_func=f_classif, k=k_value)\n",
    "        elif (score_f == 2):\n",
    "            s = SelectKBest(score_func=chi2, k=k_value)\n",
    "        elif (score_f == 3):\n",
    "            s = SelectKBest(score_func=mutual_info_classif, k=k_value)\n",
    "        elif (score_f == 4):\n",
    "            s = SelectFpr(score_func=chi2, alpha=0.01)\n",
    "        elif (score_f == 5):\n",
    "            s = SelectFdr(score_func=chi2, alpha=0.01)\n",
    "            \n",
    "        new_X = s.fit_transform(X,y)\n",
    "        columns = s.get_support(indices=True)\n",
    "        scores = s.scores_[s.get_support()]\n",
    "        new_data = np.hstack((new_X,y))\n",
    "\n",
    "        zipped_f = zip(scores,columns)\n",
    "        zipped_f = sorted(zipped_f,reverse=True)\n",
    "        sorted_columns = [column for (score,column) in zipped_f]\n",
    "    else:\n",
    "        if (score_f == 6):\n",
    "            estimator = SVR(kernel = \"linear\")\n",
    "        elif (score_f == 7):\n",
    "            estimator = LogisticRegression()\n",
    "        elif (score_f == 8):\n",
    "            estimator = RandomForestClassifier(max_depth=m_depth,random_state=0)\n",
    "         \n",
    "        print('step_value:',step_value)\n",
    "        s = RFE(estimator,n_features_to_select=k_value,step=step_value)\n",
    "\n",
    "        new_X = s.fit_transform(X,y)\n",
    "        columns = s.get_support(indices=True)\n",
    "        new_data = np.hstack((new_X,y))\n",
    "        sorted_columns = [] \n",
    "    \n",
    "    return new_data,sorted_columns\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mode=>   1:unigram,   2:dense_bigram,   3:s_unigram,   4: s_dense_bigram\n",
    "def features_names(c,mode):\n",
    "    f_names = []\n",
    "    if (mode == 1):\n",
    "        for i in range(len(c)):\n",
    "            f_names.append(words[int(c[i])])\n",
    "    \n",
    "    elif (mode == 2):\n",
    "        for i in range(len(c)):\n",
    "            bigram_name = (words[int(dense_bigram_1[int(c[i])][0])],words[int(dense_bigram_1[int(c[i])][1])])\n",
    "            f_names.append(bigram_name)\n",
    "    \n",
    "    elif (mode == 3):\n",
    "        for i in range(len(c)):\n",
    "            f_names.append(s_words[int(c[i])])\n",
    "    \n",
    "    elif (mode == 4):\n",
    "        for i in range(len(c)):\n",
    "            bigram_name = (s_words[int(s_dense_bigram_1[int(c[i])][0])],s_words[int(s_dense_bigram_1[int(c[i])][1])])\n",
    "            f_names.append(bigram_name)\n",
    "    \n",
    "    return(f_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting to binary array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_binary(array_1):\n",
    "    array_2 = np.where(array_1>0,1,0)\n",
    "    return(array_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot ROC-AUC and number of features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_2(Y_test,predicted_result):\n",
    "    c_matrix = confusion_matrix(Y_test,predicted_result)\n",
    "    tn = c_matrix[0,0]\n",
    "    fp = c_matrix[0,1]\n",
    "    fn = c_matrix[1,0]\n",
    "    tp = c_matrix[1,1]\n",
    "    \n",
    "    if ((tp + fp) == 0):\n",
    "        precision = 'not_defined'\n",
    "    else:\n",
    "        precision = tp / (tp + fp)\n",
    "    if ((tp + fn) == 0):\n",
    "        recall = 'not_defined'\n",
    "    else:\n",
    "        recall = tp / (tp + fn)\n",
    "    \n",
    "    accuracy = (tp + tn)/(tn + fp + fn + tp)\n",
    "    \n",
    "    if (precision == 'not_defined' or recall == 'not_defined' or (precision + recall) == 0):\n",
    "        f1_score = 'not_defined'\n",
    "    else:\n",
    "        f1_score = (2 * precision * recall)/(precision + recall)\n",
    "        \n",
    "        \n",
    "    if ((fp + tn) == 0):\n",
    "        fp_rate = 'not_defined'\n",
    "    else:\n",
    "        fp_rate = fp / (fp + tn)\n",
    "\n",
    "    return (precision,recall,accuracy,f1_score,fp_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression_3_2(train_1,test_1):\n",
    "    max_iter = 1000000\n",
    "\n",
    "#     threshold = 0.1623\n",
    "    \n",
    "    length = train_1.shape[1] - 1\n",
    "    X_train = train_1[:,0:length]\n",
    "    Y_train = train_1[:,length]\n",
    "    X_test = test_1[:,0:length]\n",
    "    Y_test = test_1[:,length]\n",
    "    \n",
    "    logistic_regression = LogisticRegression()\n",
    "    model = logistic_regression.fit(X_train, Y_train)\n",
    "    \n",
    "    predicted_r = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    fp_r, tp_r, t_array = roc_curve(Y_test,predicted_r)\n",
    "    \n",
    "    auc_1 = roc_auc_score(Y_test,predicted_r)\n",
    "\n",
    "    length_2 = int(predicted_r.shape[0])\n",
    "    \n",
    "    predicted_result = np.zeros(length_2)\n",
    "    for i in range(length_2):\n",
    "        if (predicted_r[i] >= threshold):\n",
    "            predicted_result[i] = 1\n",
    "        else:\n",
    "            predicted_result[i] = 0\n",
    "    \n",
    "    Y_test = Y_test.astype(int)\n",
    "    computed_metrics = compute_metrics_2(Y_test,predicted_result)\n",
    "    return computed_metrics,auc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def RF_classifier_3_2(train_1,test_1):\n",
    "#     threshold = 0.1623\n",
    "    length = train_1.shape[1] - 1\n",
    "    X_train = train_1[:,0:length]\n",
    "    Y_train = train_1[:,length]\n",
    "    X_test = test_1[:,0:length]\n",
    "    Y_test = test_1[:,length]\n",
    "    \n",
    "    clf = RandomForestClassifier(max_depth=m_depth,random_state=0)\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    predicted_r = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    fp_r, tp_r, t_array = roc_curve(Y_test,predicted_r)\n",
    "        \n",
    "    auc_1 = roc_auc_score(Y_test,predicted_r)\n",
    "\n",
    "    length_2 = int(predicted_r.shape[0])\n",
    "    \n",
    "    predicted_result = np.zeros(length_2)\n",
    "    for i in range(length_2):\n",
    "        if (predicted_r[i] >= threshold):\n",
    "            predicted_result[i] = 1\n",
    "        else:\n",
    "            predicted_result[i] = 0\n",
    "    \n",
    "    Y_test = Y_test.astype(int)\n",
    "    computed_metrics = compute_metrics_2(Y_test,predicted_result)\n",
    "    return computed_metrics,auc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def KN_classifier_3_2(train_1,test_1):\n",
    "#     threshold = 0.1623\n",
    "    length = train_1.shape[1] - 1\n",
    "    X_train = train_1[:,0:length]\n",
    "    Y_train = train_1[:,length]\n",
    "    X_test = test_1[:,0:length]\n",
    "    Y_test = test_1[:,length]\n",
    "    \n",
    "    k_value = math.floor(math.sqrt(train_1.shape[0]))\n",
    "    if (k_value % 2 == 0):\n",
    "        k_value += 1\n",
    "    else:\n",
    "        start_val = 0\n",
    "    \n",
    "    model = KNeighborsClassifier(n_neighbors = k_value)\n",
    "    model.fit(X_train, Y_train)\n",
    "\n",
    "    predicted_r = model.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    fp_r, tp_r, t_array = roc_curve(Y_test,predicted_r)\n",
    "    \n",
    "    auc_1 = roc_auc_score(Y_test,predicted_r)\n",
    "\n",
    "    length_2 = int(predicted_r.shape[0])\n",
    "    \n",
    "    predicted_result = np.zeros(length_2)\n",
    "    for i in range(length_2):\n",
    "        if (predicted_r[i] >= threshold):\n",
    "            predicted_result[i] = 1\n",
    "        else:\n",
    "            predicted_result[i] = 0\n",
    "    \n",
    "    Y_test = Y_test.astype(int)\n",
    "    computed_metrics = compute_metrics_2(Y_test,predicted_result)\n",
    "    return computed_metrics,auc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def G_NB_3_2(train_1,test_1):\n",
    "#     threshold = 0.1623\n",
    "    length = train_1.shape[1] - 1\n",
    "    X_train = train_1[:,0:length]\n",
    "    Y_train = train_1[:,length]\n",
    "    X_test = test_1[:,0:length]\n",
    "    Y_test = test_1[:,length]\n",
    "    \n",
    "    clf = GaussianNB()\n",
    "    clf.fit(X_train, Y_train)\n",
    "\n",
    "    predicted_r = clf.predict_proba(X_test)[:,1]\n",
    "    \n",
    "    fp_r, tp_r, t_array = roc_curve(Y_test,predicted_r)\n",
    "    \n",
    "    auc_1 = roc_auc_score(Y_test,predicted_r)\n",
    "\n",
    "    length_2 = int(predicted_r.shape[0])\n",
    "    \n",
    "    predicted_result = np.zeros(length_2)\n",
    "    for i in range(length_2):\n",
    "        if (predicted_r[i] >= threshold):\n",
    "            predicted_result[i] = 1\n",
    "        else:\n",
    "            predicted_result[i] = 0\n",
    "    \n",
    "    Y_test = Y_test.astype(int)\n",
    "    computed_metrics = compute_metrics_2(Y_test,predicted_result)\n",
    "    return computed_metrics,auc_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic regression => m:1\n",
    "# Sequential NN => m:3,4\n",
    "def kf_average_2(d_1,m):\n",
    "    trials_n = 1\n",
    "    precision_array = []\n",
    "    recall_array = []\n",
    "    accuracy_array = []\n",
    "    f1_score_array = []\n",
    "    fp_rate_array = []\n",
    "    auc_array = []\n",
    "    data = kf_data_split(d_1)\n",
    "    for t in range(trials_n):\n",
    "        print('\\nTrial number:',t+1)\n",
    "        for i in range(len(data)):\n",
    "            print('\\nIteration number:',(i+1))\n",
    "            train_data = data[i][0]\n",
    "            test_data = data[i][1]\n",
    "\n",
    "            if (m == 1):\n",
    "                result,auc_result = logistic_regression_3_2(train_data,test_data)\n",
    "            if (m == 2):\n",
    "                result,auc_result = RF_classifier_3_2(train_data,test_data)\n",
    "            elif (m == 3):\n",
    "                result,auc_result = sequential_3(train_data,test_data,1)\n",
    "            elif (m == 4):\n",
    "                result,auc_result = sequential_3(train_data,test_data,2)\n",
    "            elif (m == 5):\n",
    "                result,auc_result = sequential_3(train_data,test_data,3)\n",
    "            elif (m == 6):\n",
    "                result,auc_result = sequential_3(train_data,test_data,4)\n",
    "            elif (m == 7):\n",
    "                result,auc_result = sequential_3(train_data,test_data,5)\n",
    "            elif (m == 8):\n",
    "                result,auc_result = sequential_3(train_data,test_data,6)\n",
    "            elif (m == 9):\n",
    "                result,auc_result = KN_classifier_3_2(train_data,test_data)\n",
    "            elif (m == 10):\n",
    "                result,auc_result = G_NB_3_2(train_data,test_data)\n",
    "                \n",
    "            precision,recall,accuracy,f1_score,fp_rate = result\n",
    "\n",
    "            if (precision != 'not_defined'):\n",
    "                precision_array.append(precision)\n",
    "            if (recall != 'not_defined'):\n",
    "                recall_array.append(recall)\n",
    "            if (accuracy != 'not_defined'):\n",
    "                accuracy_array.append(accuracy)\n",
    "            if (f1_score != 'not_defined'):\n",
    "                f1_score_array.append(f1_score)\n",
    "            if (fp_rate != 'not_defined'):\n",
    "                fp_rate_array.append(fp_rate)\n",
    "            if (auc_result != 'not_defined'):\n",
    "                auc_array.append(auc_result)\n",
    "    \n",
    "    precision_array = np.array(precision_array)\n",
    "    recall_array = np.array(recall_array)\n",
    "    accuracy_array = np.array(accuracy_array)\n",
    "    f1_score_array = np.array(f1_score_array)\n",
    "    fp_rate_array = np.array(fp_rate_array)\n",
    "    auc_array = np.array(auc_array)\n",
    "    \n",
    "    mean_auc_value = round(auc_array.mean(),4)\n",
    "    return(mean_auc_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_values(data_1,data_2,data_3,data_4,data_5,data_6,data_7,data_8,score_f_type,c_method):\n",
    "    features_n = []\n",
    "    \n",
    "    auc_array_p_AST1 = []\n",
    "    auc_array_c_AST1 = []\n",
    "    auc_array_b_AST1 = []\n",
    "    \n",
    "    auc_array_p_AST2 = []\n",
    "    auc_array_c_AST2 = []\n",
    "    auc_array_b_AST2 = []\n",
    "    \n",
    "    auc_array_p_AST3 = []\n",
    "    auc_array_c_AST3 = []\n",
    "    auc_array_b_AST3 = []\n",
    "    \n",
    "    auc_array_p_AST4 = []\n",
    "    auc_array_c_AST4 = []\n",
    "    auc_array_b_AST4 = []\n",
    "    for i in range(25,301,25):\n",
    "        features_n.append(i)\n",
    "        \n",
    "        b_data = select_features_1(data_1,i,score_f_type)[0]\n",
    "        auc_value_p = kf_average_2(b_data,c_method)\n",
    "        auc_array_p_AST1.append(auc_value_p)\n",
    "        \n",
    "        b_data = select_features_1(data_2,i,score_f_type)[0]\n",
    "        auc_value_c = kf_average_2(b_data,c_method)\n",
    "        auc_array_c_AST1.append(auc_value_c)\n",
    "        \n",
    "        significant_b_data = select_features_1(data_2,i,score_f_type)[0]\n",
    "        b_data = convert_to_binary(significant_b_data)\n",
    "        auc_value_b = kf_average_2(b_data,c_method)\n",
    "        auc_array_b_AST1.append(auc_value_b)\n",
    "        \n",
    "        \n",
    "        \n",
    "        b_data = select_features_1(data_3,i,score_f_type)[0]\n",
    "        auc_value_p = kf_average_2(b_data,c_method)\n",
    "        auc_array_p_AST2.append(auc_value_p)\n",
    "        \n",
    "        b_data = select_features_1(data_4,i,score_f_type)[0]\n",
    "        auc_value_c = kf_average_2(b_data,c_method)\n",
    "        auc_array_c_AST2.append(auc_value_c)\n",
    "        \n",
    "        significant_b_data = select_features_1(data_4,i,score_f_type)[0]\n",
    "        b_data = convert_to_binary(significant_b_data)\n",
    "        auc_value_b = kf_average_2(b_data,c_method)\n",
    "        auc_array_b_AST2.append(auc_value_b)\n",
    "        \n",
    "        \n",
    "        \n",
    "        b_data = select_features_1(data_5,i,score_f_type)[0]\n",
    "        auc_value_p = kf_average_2(b_data,c_method)\n",
    "        auc_array_p_AST3.append(auc_value_p)\n",
    "        \n",
    "        b_data = select_features_1(data_6,i,score_f_type)[0]\n",
    "        auc_value_c = kf_average_2(b_data,c_method)\n",
    "        auc_array_c_AST3.append(auc_value_c)\n",
    "        \n",
    "        significant_b_data = select_features_1(data_6,i,score_f_type)[0]\n",
    "        b_data = convert_to_binary(significant_b_data)\n",
    "        auc_value_b = kf_average_2(b_data,c_method)\n",
    "        auc_array_b_AST3.append(auc_value_b)\n",
    "        \n",
    "        \n",
    "        \n",
    "        b_data = select_features_1(data_7,i,score_f_type)[0]\n",
    "        auc_value_p = kf_average_2(b_data,c_method)\n",
    "        auc_array_p_AST4.append(auc_value_p)\n",
    "        \n",
    "        b_data = select_features_1(data_8,i,score_f_type)[0]\n",
    "        auc_value_c = kf_average_2(b_data,c_method)\n",
    "        auc_array_c_AST4.append(auc_value_c)\n",
    "        \n",
    "        significant_b_data = select_features_1(data_8,i,score_f_type)[0]\n",
    "        b_data = convert_to_binary(significant_b_data)\n",
    "        auc_value_b = kf_average_2(b_data,c_method)\n",
    "        auc_array_b_AST4.append(auc_value_b)\n",
    "        \n",
    "    return(features_n,auc_array_p_AST1,auc_array_c_AST1,auc_array_b_AST1,auc_array_p_AST2,auc_array_c_AST2,auc_array_b_AST2,auc_array_p_AST3,auc_array_c_AST3,auc_array_b_AST3,auc_array_p_AST4,auc_array_c_AST4,auc_array_b_AST4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_f_type = 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression\n",
    "result = compute_values(d_1_p,d_1_c,d_2_p,d_2_c,d_3_p,d_3_c,d_4_p,d_4_c,score_f_type,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest\n",
    "m_depth = 20\n",
    "result = compute_values(d_1_p,d_1_c,d_2_p,d_2_c,d_3_p,d_3_c,d_4_p,d_4_c,score_f_type,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# KNN\n",
    "result = compute_values(d_1_p,d_1_c,d_2_p,d_2_c,d_3_p,d_3_c,d_4_p,d_4_c,score_f_type,9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gaussian Naive Bayes\n",
    "result = compute_values(d_1_p,d_1_c,d_2_p,d_2_c,d_3_p,d_3_c,d_4_p,d_4_c,score_f_type,10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE Logistic Regression\n",
    "result = compute_values_RFE(d_1_p,d_1_c,d_2_p,d_2_c,d_3_p,d_3_c,d_4_p,d_4_c,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_n,auc_array_p_AST1,auc_array_c_AST1,auc_array_b_AST1,auc_array_p_AST2,auc_array_c_AST2,auc_array_b_AST2,auc_array_p_AST3,auc_array_c_AST3,auc_array_b_AST3,auc_array_p_AST4,auc_array_c_AST4,auc_array_b_AST4 = result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    " \n",
    "plt.plot(features_n,auc_array_p_AST1,color='red',marker='o',label='Probability_AST1')\n",
    "plt.plot(features_n,auc_array_c_AST1,color='red',marker='o',linestyle='--',dashes=(3,3),label='Count_AST1')\n",
    "plt.plot(features_n,auc_array_b_AST1,color='red',marker='o',linestyle='dotted',label='Binary_AST1')\n",
    "\n",
    "plt.plot(features_n,auc_array_p_AST2,color='black',marker='o',label='Probability_AST2')\n",
    "plt.plot(features_n,auc_array_c_AST2,color='black',marker='o',linestyle='--',dashes=(3,3),label='Count_AST2')\n",
    "plt.plot(features_n,auc_array_b_AST2,color='black',marker='o',linestyle='dotted',label='Binary_AST2')\n",
    "\n",
    "plt.plot(features_n,auc_array_p_AST3,color='skyblue',marker='o',label='Probability_AST3')\n",
    "plt.plot(features_n,auc_array_c_AST3,color='skyblue',marker='o',linestyle='--',dashes=(3,3),label='Count_AST3')\n",
    "plt.plot(features_n,auc_array_b_AST3,color='skyblue',marker='o',linestyle='dotted',label='Binary_AST3')\n",
    "\n",
    "plt.plot(features_n,auc_array_p_AST4,color='blue',marker='o',label='Probability_AST4')\n",
    "plt.plot(features_n,auc_array_c_AST4,color='blue',marker='o',linestyle='--',dashes=(3,3),label='Count_AST4')\n",
    "plt.plot(features_n,auc_array_b_AST4,color='blue',marker='o',linestyle='dotted',label='Binary_AST4')\n",
    "\n",
    "plt.title('Diagram of ROC-AUC and number of features',fontsize=16)\n",
    "plt.xlabel('Number of features',fontsize=14)\n",
    "plt.ylabel('ROC-AUC',fontsize=14)\n",
    "plt.legend(fontsize=11,loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_4_c.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1_p = np.load('./data_2/dense_trigram_data_AST1.npy')\n",
    "d_1_c = np.load('./data_2/dense_trigram_data_c_AST1.npy')\n",
    "\n",
    "d_2_p = np.load('./data_2/dense_trigram_data_AST2.npy')\n",
    "d_2_c = np.load('./data_2/dense_trigram_data_c_AST2.npy')\n",
    "\n",
    "d_3_p = np.load('./data_2/dense_trigram_data_AST3.npy')\n",
    "d_3_c = np.load('./data_2/dense_trigram_data_c_AST3.npy')\n",
    "\n",
    "d_4_p = np.load('./data_2/dense_trigram_data_AST4.npy')\n",
    "d_4_c = np.load('./data_2/dense_trigram_data_c_AST4.npy')\n",
    "\n",
    "d_5_p = np.load('./data_2/dense_trigram_data.npy')\n",
    "d_5_c = np.load('./data_2/dense_trigram_data.npy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Selecting subset of data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_values_RFE(data_1,data_2,data_3,data_4,data_5,data_6,data_7,data_8,data_9,data_10,c_method):\n",
    "    d_1 =data_1.copy()\n",
    "    d_2 =data_2.copy()\n",
    "    d_3 =data_3.copy()\n",
    "    d_4 =data_4.copy()\n",
    "    d_5 =data_5.copy()\n",
    "    d_6 =data_6.copy()\n",
    "    d_7 =data_7.copy()\n",
    "    d_8 =data_8.copy()\n",
    "    d_9 =data_9.copy()\n",
    "    d_10 =data_10.copy()\n",
    "    \n",
    "    features_n = []\n",
    "    \n",
    "    auc_array_p_AST1 = []\n",
    "    auc_array_c_AST1 = []\n",
    "    auc_array_b_AST1 = []\n",
    "    \n",
    "    auc_array_p_AST2 = []\n",
    "    auc_array_c_AST2 = []\n",
    "    auc_array_b_AST2 = []\n",
    "    \n",
    "    auc_array_p_AST3 = []\n",
    "    auc_array_c_AST3 = []\n",
    "    auc_array_b_AST3 = []\n",
    "    \n",
    "    auc_array_p_AST4 = []\n",
    "    auc_array_c_AST4 = []\n",
    "    auc_array_b_AST4 = []\n",
    "    \n",
    "    auc_array_p_word = []\n",
    "    auc_array_c_word = []\n",
    "    auc_array_b_word = []\n",
    "    \n",
    "    if (c_method == 1):\n",
    "        score_f_type = 7\n",
    "    elif (c_method == 2):\n",
    "        score_f_type = 8\n",
    "        \n",
    "    for i in range(250,24,-25):\n",
    "#     for i in range(150,49,-50):\n",
    "        features_n.append(i)\n",
    "        \n",
    "        d_1 = select_features_2(d_1,i,score_f_type)[0]\n",
    "        auc_value_p = kf_average_2(d_1,c_method)\n",
    "        auc_array_p_AST1.append(auc_value_p)\n",
    "        \n",
    "        d_2 = select_features_2(d_2,i,score_f_type)[0]\n",
    "        auc_value_c = kf_average_2(d_2,c_method)\n",
    "        auc_array_c_AST1.append(auc_value_c)\n",
    "        \n",
    "        significant_b_data = select_features_2(d_2,i,score_f_type)[0]\n",
    "        b_data = convert_to_binary(significant_b_data)\n",
    "        auc_value_b = kf_average_2(b_data,c_method)\n",
    "        auc_array_b_AST1.append(auc_value_b)\n",
    "        \n",
    "        \n",
    "        \n",
    "        d_3 = select_features_2(d_3,i,score_f_type)[0]\n",
    "        auc_value_p = kf_average_2(d_3,c_method)\n",
    "        auc_array_p_AST2.append(auc_value_p)\n",
    "        \n",
    "        d_4 = select_features_2(d_4,i,score_f_type)[0]\n",
    "        auc_value_c = kf_average_2(d_4,c_method)\n",
    "        auc_array_c_AST2.append(auc_value_c)\n",
    "        \n",
    "        b_data = convert_to_binary(d_4)\n",
    "        auc_value_b = kf_average_2(b_data,c_method)\n",
    "        auc_array_b_AST2.append(auc_value_b)\n",
    "        \n",
    "        \n",
    "        \n",
    "        d_5 = select_features_2(d_5,i,score_f_type)[0]\n",
    "        auc_value_p = kf_average_2(d_5,c_method)\n",
    "        auc_array_p_AST3.append(auc_value_p)\n",
    "        \n",
    "        d_6 = select_features_2(d_6,i,score_f_type)[0]\n",
    "        auc_value_c = kf_average_2(d_6,c_method)\n",
    "        auc_array_c_AST3.append(auc_value_c)\n",
    "        \n",
    "        b_data = convert_to_binary(d_6)\n",
    "        auc_value_b = kf_average_2(b_data,c_method)\n",
    "        auc_array_b_AST3.append(auc_value_b)\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(d_7.shape)\n",
    "        d_7 = select_features_2(d_7,i,score_f_type)[0]\n",
    "        print(d_7.shape)\n",
    "        auc_value_p = kf_average_2(d_7,c_method)\n",
    "        auc_array_p_AST4.append(auc_value_p)\n",
    "        \n",
    "        d_8 = select_features_2(d_8,i,score_f_type)[0]\n",
    "        auc_value_c = kf_average_2(d_8,c_method)\n",
    "        auc_array_c_AST4.append(auc_value_c)\n",
    "        \n",
    "        b_data = convert_to_binary(d_8)\n",
    "        auc_value_b = kf_average_2(b_data,c_method)\n",
    "        auc_array_b_AST4.append(auc_value_b)\n",
    "        \n",
    "        \n",
    "        \n",
    "        d_9 = select_features_2(d_9,i,score_f_type)[0]\n",
    "        auc_value_p = kf_average_2(d_9,c_method)\n",
    "        auc_array_p_word.append(auc_value_p)\n",
    "        \n",
    "        d_10 = select_features_2(d_10,i,score_f_type)[0]\n",
    "        auc_value_c = kf_average_2(d_10,c_method)\n",
    "        auc_array_c_word.append(auc_value_c)\n",
    "        \n",
    "        b_data = convert_to_binary(d_10)\n",
    "        auc_value_b = kf_average_2(b_data,c_method)\n",
    "        auc_array_b_word.append(auc_value_b)\n",
    "        \n",
    "    return(features_n,auc_array_p_AST1,auc_array_c_AST1,auc_array_b_AST1,auc_array_p_AST2,auc_array_c_AST2,auc_array_b_AST2,auc_array_p_AST3,auc_array_c_AST3,auc_array_b_AST3,auc_array_p_AST4,auc_array_c_AST4,auc_array_b_AST4,auc_array_p_word,auc_array_c_word,auc_array_b_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_data_subset(data_1,c):\n",
    "    start_time = time.time()\n",
    "    if (c == 1):\n",
    "        new_d = select_features_2(data_1,400,7)[0]\n",
    "    elif (c == 2):\n",
    "        new_d = select_features_2(data_1,400,8)[0]\n",
    "    finish_time = time.time()\n",
    "    elapsed_time = finish_time - start_time\n",
    "    print(elapsed_time)\n",
    "    return new_d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1:LR,   2:RF,   9:KNN,   10:GNB\n",
    "c1 = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_value = 25\n",
    "d_1_p_2 = select_data_subset(d_1_p,c1)\n",
    "d_1_c_2 = select_data_subset(d_1_c,c1)\n",
    "\n",
    "step_value = 25\n",
    "d_2_p_2 = select_data_subset(d_2_p,c1)\n",
    "d_2_c_2 = select_data_subset(d_2_c,c1)\n",
    "\n",
    "step_value = 25\n",
    "d_3_p_2 = select_data_subset(d_3_p,c1)\n",
    "d_3_c_2 = select_data_subset(d_3_c,c1)\n",
    "\n",
    "step_value = 25\n",
    "d_4_p_2 = select_data_subset(d_4_p,c1)\n",
    "d_4_c_2 = select_data_subset(d_4_c,c1)\n",
    "\n",
    "step_value = 25\n",
    "d_5_p_2 = select_data_subset(d_5_p,c1)\n",
    "d_5_c_2 = select_data_subset(d_5_c,c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_value = 25\n",
    "d_1_p_2 = select_data_subset(d_1_p,c1)\n",
    "d_1_c_2 = select_data_subset(d_1_c,c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_value = 25\n",
    "d_2_p_2 = select_data_subset(d_2_p,c1)\n",
    "d_2_c_2 = select_data_subset(d_2_c,c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_value = 25\n",
    "d_3_p_2 = select_data_subset(d_3_p,c1)\n",
    "d_3_c_2 = select_data_subset(d_3_c,c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_value = 25\n",
    "d_4_p_2 = select_data_subset(d_4_p,c1)\n",
    "d_4_c_2 = select_data_subset(d_4_c,c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_value = 25\n",
    "d_5_p_2 = select_data_subset(d_5_p,c1)\n",
    "d_5_c_2 = select_data_subset(d_5_c,c1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE Logistic Regression\n",
    "step_value = 25\n",
    "result = compute_values_RFE(d_1_p_2, d_1_c_2, d_2_p_2, d_2_c_2, d_3_p_2, d_3_c_2, d_4_p_2, d_4_c_2, d_5_p_2, d_5_c_2, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFE Random Forest\n",
    "step_value = 25\n",
    "m_depth = 10\n",
    "result = compute_values_RFE(d_1_p_2, d_1_c_2, d_2_p_2, d_2_c_2, d_3_p_2, d_3_c_2, d_4_p_2, d_4_c_2, d_5_p_2, d_5_c_2, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_n,auc_array_p_AST1,auc_array_c_AST1,auc_array_b_AST1,auc_array_p_AST2,auc_array_c_AST2,auc_array_b_AST2,auc_array_p_AST3,auc_array_c_AST3,auc_array_b_AST3,auc_array_p_AST4,auc_array_c_AST4,auc_array_b_AST4,auc_array_p_word,auc_array_c_word,auc_array_b_word = result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in result:\n",
    "    print(np.max(r))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = (10,6)\n",
    " \n",
    "plt.plot(features_n,auc_array_p_AST1,color='red',marker='o',label='Probability_AST1')\n",
    "plt.plot(features_n,auc_array_c_AST1,color='red',marker='o',linestyle='--',dashes=(3,3),label='Count_AST1')\n",
    "plt.plot(features_n,auc_array_b_AST1,color='red',marker='o',linestyle='dotted',label='Binary_AST1')\n",
    "\n",
    "plt.plot(features_n,auc_array_p_AST2,color='black',marker='o',label='Probability_AST2')\n",
    "plt.plot(features_n,auc_array_c_AST2,color='black',marker='o',linestyle='--',dashes=(3,3),label='Count_AST2')\n",
    "plt.plot(features_n,auc_array_b_AST2,color='black',marker='o',linestyle='dotted',label='Binary_AST2')\n",
    "\n",
    "plt.plot(features_n,auc_array_p_AST3,color='skyblue',marker='o',label='Probability_AST3')\n",
    "plt.plot(features_n,auc_array_c_AST3,color='skyblue',marker='o',linestyle='--',dashes=(3,3),label='Count_AST3')\n",
    "plt.plot(features_n,auc_array_b_AST3,color='skyblue',marker='o',linestyle='dotted',label='Binary_AST3')\n",
    "\n",
    "plt.plot(features_n,auc_array_p_AST4,color='blue',marker='o',label='Probability_AST4')\n",
    "plt.plot(features_n,auc_array_c_AST4,color='blue',marker='o',linestyle='--',dashes=(3,3),label='Count_AST4')\n",
    "plt.plot(features_n,auc_array_b_AST4,color='blue',marker='o',linestyle='dotted',label='Binary_AST4')\n",
    "\n",
    "plt.plot(features_n,auc_array_p_word,color='grey',marker='o',label='Probability_word')\n",
    "plt.plot(features_n,auc_array_c_word,color='grey',marker='o',linestyle='--',dashes=(3,3),label='Count_word')\n",
    "plt.plot(features_n,auc_array_b_word,color='grey',marker='o',linestyle='dotted',label='Binary_word')\n",
    "\n",
    "plt.title('Diagram of ROC-AUC and number of features',fontsize=16)\n",
    "plt.xlabel('Number of features',fontsize=14)\n",
    "plt.ylabel('ROC-AUC',fontsize=14)\n",
    "# plt.legend(fontsize=11,loc='lower right')\n",
    "plt.gca().legend(loc='center left', bbox_to_anchor=(1,0.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('./data_2/dense_trigram_data_AST1_LR.npy',d_1_p_2)\n",
    "np.save('./data_2/dense_trigram_data_c_AST1_LR.npy',d_1_c_2)\n",
    "\n",
    "np.save('./data_2/dense_trigram_data_AST2_LR.npy',d_2_p_2)\n",
    "np.save('./data_2/dense_trigram_data_c_AST2_LR.npy',d_2_c_2)\n",
    "\n",
    "np.save('./data_2/dense_trigram_data_AST3_LR.npy',d_3_p_2)\n",
    "np.save('./data_2/dense_trigram_data_c_AST3_LR.npy',d_3_c_2)\n",
    "\n",
    "np.save('./data_2/dense_trigram_data_AST4_LR.npy',d_4_p_2)\n",
    "np.save('./data_2/dense_trigram_data_c_AST4_LR.npy',d_4_c_2)\n",
    "\n",
    "np.save('./data_2/dense_trigram_data_word_LR.npy',d_5_p_2)\n",
    "np.save('./data_2/dense_trigram_data_c_word_LR.npy',d_5_c_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_1_p_2 = np.load('./data_2/dense_trigram_data_AST1_LR.npy')\n",
    "d_1_c_2 = np.load('./data_2/dense_trigram_data_c_AST1_LR.npy')\n",
    "\n",
    "d_2_p_2 = np.load('./data_2/dense_trigram_data_AST2_LR.npy')\n",
    "d_2_c_2 = np.load('./data_2/dense_trigram_data_c_AST2_LR.npy')\n",
    "\n",
    "d_3_p_2 = np.load('./data_2/dense_trigram_data_AST3_LR.npy')\n",
    "d_3_c_2 = np.load('./data_2/dense_trigram_data_c_AST3_LR.npy')\n",
    "\n",
    "d_4_p_2 = np.load('./data_2/dense_trigram_data_AST4_LR.npy')\n",
    "d_4_c_2 = np.load('./data_2/dense_trigram_data_c_AST4_LR.npy')\n",
    "\n",
    "d_5_p_2 = np.load('./data_2/dense_trigram_data_word_LR.npy')\n",
    "d_5_c_2 = np.load('./data_2/dense_trigram_data_c_word_LR.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(d_5_p_2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
